[0;32mStarting B200 Discovery Script (Verified Edition)[0m
Log file: /home/ubuntu/b200_discovery_192-9-179-49_20251218_205947.log
Started at: Thu Dec 18 20:59:47 UTC 2025
Running as: ubuntu


===============================================================================
  PHASE 1: QUICK SYSTEM SNAPSHOT
  2025-12-18 20:59:47
===============================================================================


--- Basic System Info ---

[CMD] Hostname: hostname
192-9-179-49

[CMD] Kernel: uname -a
Linux 192-9-179-49 6.8.0-60-generic #63~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Apr 22 19:00:15 UTC 2 x86_64 x86_64 x86_64 GNU/Linux

[CMD] OS Release: cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

[CMD] Architecture: uname -m
x86_64

[CMD] Uptime: uptime
 20:59:47 up 7 min,  1 user,  load average: 0.06, 0.06, 0.02

[CMD] Current user: whoami
ubuntu

[CMD] User groups: groups
ubuntu users admin


--- Kernel Type Analysis ---

[INFO] Checking if NVIDIA-optimized kernel vs generic kernel
[INFO] Generic Ubuntu kernel: 6.8.0-60-generic
     Note: Should work fine but lacks NVIDIA-specific optimizations


--- Virtualization Detection ---

[CMD] Virtualization: systemd-detect-virt
kvm

[CMD] Hypervisor info: cat /sys/hypervisor/type
cat: /sys/hypervisor/type: No such file or directory
[WARN] Command returned non-zero exit code

[CMD] DMI product: cat /sys/class/dmi/id/product_name
Standard PC (Q35 + ICH9, 2009)

[CMD] CPU flags (vmx/svm): grep -E vmx|svm /proc/cpuinfo
No virtualization flags

--- Quick GPU Check ---

[CMD] nvidia-smi overview: nvidia-smi
Thu Dec 18 20:59:47 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.148.08             Driver Version: 570.148.08     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA B200                    On  |   00000000:06:00.0 Off |                    0 |
| N/A   33C    P0            141W / 1000W |       0MiB / 183359MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

[CMD] GPU count and names: nvidia-smi -L
GPU 0: NVIDIA B200 (UUID: GPU-8741c41e-32f5-8eb1-fa98-6ab21942fa2f)


--- Memory Overview ---

[CMD] Memory: free -h
               total        used        free      shared  buff/cache   available
Mem:           354Gi       860Mi       350Gi       5.0Mi       2.3Gi       350Gi
Swap:             0B          0B          0B

[CMD] Swap: swapon --show


===============================================================================
  PHASE 2: B200 BLACKWELL CRITICAL CHECKS
  2025-12-18 20:59:47
===============================================================================


--- Driver Type Verification (B200 REQUIRES nvidia-open) ---

[CRITICAL] B200/Blackwell ONLY works with nvidia-open kernel modules
[CRITICAL] Proprietary driver does NOT support Blackwell architecture

[FILE] /proc/driver/nvidia/version:
NVRM version: NVIDIA UNIX Open Kernel Module for x86_64  570.148.08  Release Build  (dvs-builder@U22-I3-AE18-23-3)  Wed May 21 07:03:28 UTC 2025
GCC version:  gcc version 12.3.0 (Ubuntu 12.3.0-1ubuntu1~22.04) 

[CMD] Kernel modules (nvidia): lsmod
nvidia_uvm           2121728  0
nvidia_drm            131072  0
nvidia_modeset       1724416  1 nvidia_drm
video                  77824  1 nvidia_modeset
nvidia_peermem         16384  0
ib_uverbs             200704  3 nvidia_peermem,rdma_ucm,mlx5_ib
nvidia              11644928  8 nvidia_uvm,nvidia_peermem,nvidia_modeset
ecc                    45056  1 nvidia
filename:       /lib/modules/6.8.0-60-generic/updates/dkms/nvidia.ko
version:        570.148.08
license:        Dual MIT/GPL
[WARN] Could not confirm nvidia-open - B200 requires open kernel modules


--- DKMS Status (Kernel Module Build System) ---

[INFO] DKMS manages kernel module compilation for different kernels
[CMD] DKMS status: dkms status
iser/24.10.OFED.24.10.2.1.8.1, 6.8.0-60-generic, x86_64: installed
isert/24.10.OFED.24.10.2.1.8.1, 6.8.0-60-generic, x86_64: installed
kernel-mft-dkms/4.30.1.113, 6.8.0-60-generic, x86_64: installed
knem/1.1.4.90mlnx3, 6.8.0-60-generic, x86_64: installed
mlnx-ofed-kernel/24.10.OFED.24.10.2.1.8.1, 6.8.0-60-generic, x86_64: installed
nvidia-srv/570.148.08, 6.8.0-60-generic, x86_64: installed
srp/24.10.OFED.24.10.2.1.8.1, 6.8.0-60-generic, x86_64: installed
xpmem/2.7.4, 6.8.0-60-generic, x86_64: installed

[CMD] DKMS nvidia modules: dkms status
nvidia-srv/570.148.08, 6.8.0-60-generic, x86_64: installed

--- Kernel Logs (NVIDIA driver initialization) ---

[INFO] Checking kernel logs for NVIDIA driver messages
[CMD] dmesg nvidia (last 30): dmesg
[CMD] journalctl nvidia: journalctl -k

--- Fabric Manager Status (REQUIRED for NVSwitch on B200) ---

[INFO] B200 HGX systems require Fabric Manager + NVLSM for NVSwitch operation

[SERVICE] nvidia-fabricmanager:
inactive
inactive/not found
‚óã nvidia-fabricmanager.service - NVIDIA fabric manager service
     Loaded: loaded (/lib/systemd/system/nvidia-fabricmanager.service; disabled; vendor preset: enabled)
     Active: inactive (dead)
[SKIP] Service not found

[SERVICE] nvidia-nvlsm:
inactive
inactive/not found
[SKIP] Service not found

[SERVICE] nvidia-persistenced:
active
‚óè nvidia-persistenced.service - NVIDIA Persistence Daemon
     Loaded: loaded (/lib/systemd/system/nvidia-persistenced.service; static)
     Active: active (running) since Thu 2025-12-18 20:53:01 UTC; 6min ago
   Main PID: 2027 (nvidia-persiste)
      Tasks: 1 (limit: 434832)
     Memory: 964.0K
        CPU: 2.481s
     CGroup: /system.slice/nvidia-persistenced.service
             ‚îî‚îÄ2027 /usr/bin/nvidia-persistenced --user nvidia-persistenced --persistence-mode --verbose

[CMD] Fabric Manager version: nv-fabricmanager --version
Fabric Manager version is : 570.148.08

[FILE] /usr/share/nvidia/nvswitch/fabricmanager.cfg:
# NVIDIA Fabric Manager configuration file.
# Note: This configuration file is read during Fabric Manager service startup. So, Fabric Manager 
# service restart is required for new settings to take effect.

#	Description: Fabric Manager logging levels
#	Possible Values:
#		0  - All the logging is disabled
#		1  - Set log level to CRITICAL and above
#		2  - Set log level to ERROR and above
#		3  - Set log level to WARNING and above
#		4  - Set log level to INFO and above
LOG_LEVEL=4

#	Description: Filename for Fabric Manager logs
#	Possible Values:
#       Full path/filename string (max length of 256). Logs will be redirected to console(stderr)
#       if the specified log file can't be opened or the path is empty.
LOG_FILE_NAME=/var/log/fabricmanager.log

#	Description: Append to an existing log file or overwrite the logs
#	Possible Values:
#		0  - No  (Log file will be overwritten)
#		1  - Yes (Append to existing log)
LOG_APPEND_TO_LOG=1

#	Description: Max size of log file (in MB)
#	Possible Values:
#		Any Integer values
LOG_FILE_MAX_SIZE=1024

#   Description: Number of times the FM log is rotated once it reaches LOG_FILE_MAX_SIZE
#   Possible Values:
#       0                - Log is not rotated. Logging is stopped once the FM log file reaches
#                          the size specified in LOG_FILE_MAX_SIZE
#       Non-zero Integer - Log is rotated upto the number of times specified in LOG_MAX_ROTATE_COUNT,
#                          after the size of the log file reaches the size specified in LOG_FILE_MAX_SIZE.
#                          Combined FM log size is LOG_FILE_MAX_SIZE multipled by LOG_MAX_ROTATE_COUNT+1
#                          Once this threshold is reached, the oldest log file is purged and reused.
LOG_MAX_ROTATE_COUNT=3

#	Description: Redirect all the logs to syslog instead of logging to file
#	Possible Values:
#		0  - No
#		1  - Yes
LOG_USE_SYSLOG=0

#	Description: daemonize Fabric Manager on start-up
#	Possible Values:
#       0  - No (Do not daemonize and run fabric manager as a normal process)
#       1  - Yes (Run Fabric Manager process as Unix daemon
DAEMONIZE=1

#	Description: Network interface to listen for Global and Local Fabric Manager communication
#	Possible Values:
#		A valid IPv4 address. By default, uses loopback (127.0.0.1) interface
#   Note: This is only effective on DGX A100, HGX A100, DGX H100, HGX H100 NVSwitch based systems
BIND_INTERFACE_IP=127.0.0.1

#	Description: Starting TCP port number for Global and Local Fabric Manager communication
#	Possible Values:
#		Any value between 0 and 65535
#   Note: This is only effective on DGX A100, HGX A100, DGX H100, HGX H100 NVSwitch based systems
STARTING_TCP_PORT=16000

#   Description: Use Unix sockets instead of TCP Socket for Global and Local Fabric Manager communication
#	Possible Values:
#		Unix domain socket path (max length of 256)
#	Default Value: 
#		Empty String (TCP socket will be used instead of Unix sockets)
#   Note: This is only effective on DGX A100, HGX A100, DGX H100, HGX H100 NVSwitch based systems
UNIX_SOCKET_PATH=

#	Description: Fabric Manager Operating Mode
#	Possible Values:
#       0  - Start Fabric Manager in Bare metal or Full pass through virtualization mode
#       1  - Start Fabric Manager in Shared NVSwitch multitenancy mode. 
#       2  - Start Fabric Manager in vGPU based multitenancy mode.
FABRIC_MODE=0

#	Description: Restart Fabric Manager after exit
#                On DGX A100, HGX A100, DGX H100, HGX H100 NVSwitch based systems, this is applicable only in Shared
#                NVSwitch or vGPU based multitenancy mode
#	Possible Values:
#       0  - Start Fabric Manager and follow full initialization sequence
#       1  - Start Fabric Manager and follow resiliency/restart sequence
FABRIC_MODE_RESTART=0

#	Description: Specify the filename to be used to save Fabric Manager states.
#                    Valid only if Shared NVSwitch or vGPU based multitenancy mode is enabled
#	Possible Values:
#	    Full path/filename string (max length of 256)
#   Note: This is only effective on DGX A100, HGX A100, DGX H100, HGX H100 NVSwitch based systems
STATE_FILE_NAME=/tmp/fabricmanager.state

#	Description: Network interface to listen for Fabric Manager SDK/API to communicate with running FM instance.
#	Possible Values:
#		A valid IPv4 address. By default, uses loopback (127.0.0.1) interface
FM_CMD_BIND_INTERFACE=127.0.0.1 

#	Description: TCP port number for Fabric Manager SDK/API to communicate with running FM instance.
#	Possible Values:
#		Any value between 0 and 65535
FM_CMD_PORT_NUMBER=6666

#	Description: Use Unix sockets instead of TCP Socket for Fabric Manager SDK/API communication
#	Possible Values:
#		Unix domain socket path (max length of 256)
#	Default Value: 
#		Empty String (TCP socket will be used instead of Unix sockets)
FM_CMD_UNIX_SOCKET_PATH=

#   Description: Fabric Manager does not exit when facing failures
#   Possible Values:
#       0 ‚Äì Fabric Manager service will terminate on errors such as, NVSwitch and GPU config failure, 
#           typical software errors etc.  
#       1 ‚Äì Fabric Manager service will stay running on errors such as, NVSwitch and GPU config failure, 
#           typical software errors etc. However, the system will be uninitialized and CUDA application 
#           launch will fail. 
FM_STAY_RESIDENT_ON_FAILURES=0

#   Description: Degraded Mode options when there is an Access Link Failure (GPU to NVSwitch NVLink failure)
#   Possible Values:
#       In bare metal or full passthrough virtualization mode
#       0  - Remove the GPU with the Access NVLink failure from NVLink P2P capability
#       1  - Disable the NVSwitch and its peer NVSwitch, which reduces NVLink P2P bandwidth
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
#
#       In Shared NVSwitch or vGPU based multitenancy mode
#       0  - Disable partitions which are using the Access Link failed GPU
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
#       1  - Disable the NVSwitch and its peer NVSwitch,
#            all partitions will be available but with reduced NVLink P2P bandwidth
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
ACCESS_LINK_FAILURE_MODE=0

#   Description: Degraded Mode options when there is a Trunk Link Failure (NVSwitch to NVSwitch NVLink failure)
#   Possible Values:
#       In bare metal or full passthrough virtualization mode
#       0  - Exit Fabric Manager and leave the system/NVLinks uninitialized
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
#       1  - Disable the NVSwitch and its peer NVSwitch, which reduces NVLink P2P bandwidth
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
#
#       In Shared NVSwitch or vGPU based multitenancy mode
#       0  - Remove partitions that are using the Trunk NVLinks
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
#       1  - Disable the NVSwitch and its peer NVSwitch,
#            all partitions will be available but with reduced NVLink P2P bandwidth
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
TRUNK_LINK_FAILURE_MODE=0

#   Description: Degraded Mode options when there is a NVSwitch failure or an NVSwitch is excluded
#   Possible Values:
#       In bare metal or full passthrough virtualization mode
#       0  - Abort Fabric Manager
#       1  - Disable the NVSwitch and its peer NVSwitch, which reduces P2P bandwidth
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
#
#       In Shared NVSwitch or vGPU based multitenancy mode
#       0  - Disable partitions that are using the NVSwitch
#            This is only effective on DGX A100 and HGX A100 NVSwitch based systems
#       1  - Disable the NVSwitch and its peer NVSwitch,
#            all partitions will be available but with reduced NVLink P2P bandwidth
#            Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
NVSWITCH_FAILURE_MODE=0

#	Description: Control running CUDA jobs behavior when Fabric Manager service is stopped or terminated
#	Possible Values:
#       0  - Do not abort running CUDA jobs when Fabric Manager exits. However new CUDA job launch will fail.
#       1  - Abort all running CUDA jobs when Fabric Manager exits.
#	Note: This is only effective on DGX A100 and HGX A100 NVSwitch based systems
ABORT_CUDA_JOBS_ON_FM_EXIT=1

#       Description: Absolute directory path containing Fabric Manager topology files
#       Possible Values:
#               A valid directory path string (max length of 256)
TOPOLOGY_FILE_PATH=/usr/share/nvidia/nvswitch

#   Description: Use RPC instead of raw sockets for communication between global and local fabric manager.
#   Possible values:
#       0 - Do not use RPCs
#       1 - Use RPC
USE_RPC=1

#  Description: Name of the network interface used for communication.
#               OPTIONAL - If empty, network interface will be determined by matching bind IP to
#                          node configuration file.  Only necessary to configure if the bind IP
#                          is present on multiple network interfaces.
#  Possible Values:
#      Interface names like eth0, ens32 .. etc
#  Default Value:
NETWORK_INTERFACE=


#  Description:  Enable authentication and encryption for RPC communication.
#                NOTE: If USE_RPC is 0, this is ignored.
#  Possible Values:
#      0:  Disable encryption and authentication
#      1:  Enable encryption and authentication
#  Default value: 0
ENABLE_AUTH_ENCRYPTION=0

#  Description:  This determines how fabric manager will try to retrieve the keys, certificates, and certificate
#                authority for authentication and encryption.
#                If ENABLE_AUTH_ENCRYPTION is enabled (1), then AUTH_SOURCE must be configured
#                as one of the supported values.  An empty or unexpected value will prevent initialization.
#                If USE_RPC is 0, this is ignored.
#  Possible Values:
#      FILE:      The provided values are paths to files on the file system.
#      ENV_PATH:  The provided values are environment variable names to retrieve, and the values in the
#                 environment variables are treated as paths to files on the file system.
#      ENV_VAL:   The provided values are environment variable names to retrieve, and the values in the
#                 environment variables are treated as the actual values for the key/cert/cert auth.
AUTH_SOURCE=

# Description:  These fields are interpreted based on how AUTH_SOURCE is configured
SERVER_KEY=
SERVER_CERT=
SERVER_CERT_AUTH=
CLIENT_KEY=
CLIENT_CERT=
CLIENT_CERT_AUTH=

#  Description:  Override the target hostname for authentication of the certificates and keys.  This allows
#                certificates with common names that do not match the ip addresses provided for the nodes.
#                Example:
#                  If the certificate has the subject:
#                    "/C=US/ST=CA/L=Santa Clara/O=NVIDIA/OU=Test/CN=localhost"
#                  The certificate validation will expect the connection hostname to be "localhost", by
#                  setting IMEX_SECURITY_TARGET_OVERRIDE=localhost you can cause override the connection
#                  hostname for security purposes to be "localhost", allowing the connection to succeed.
#                If USE_RPC is 0, this is ignored.
SECURITY_TARGET_OVERRIDE=

#  Description: This tunable determines the domain behavior in case the trunk links are down or experience
#               fatal errors. When MNNVL_RESILIENCY_MODE is set to 0, in case a single trunk link fails, the
#               inter-node traffic between the impacted L1 switch node and the rest of the domain will be
#               unavailable. When MNNVL_RESILIENCY_MODE is set to 1, the domain can sustain up to failures
#               of half (excluded) of the trunk links on an L1 switch before the inter-node traffic between
#               this L1 switch node and the rest of the domain becomes unavailable.
MNNVL_RESILIENCY_MODE=0

#	Description: Determine whether a default partition needs to be created
#	Possible Values:
#       0             - No partitions are created during GFM initialization. GFM disables routing until an API request 
#                     to create a partition is successful.
#       1(default)    - Creates a default partition during GFM initialization. GFM creates the partition to include
#                     all GPUs in the topology and enables routing so that all GPUs can communicate to each other.
    
MNNVL_ENABLE_DEFAULT_PARTITION=1


--- Driver and CUDA Version Check ---

[INFO] B200 requires: Driver 570.133.20+, CUDA 12.8+ for SM_100

[CMD] nvidia-smi driver info: nvidia-smi --query-gpu=driver_version --format=csv,noheader
[CMD] CUDA version from nvidia-smi: nvidia-smi --query-gpu=name,driver_version --format=csv
name, driver_version
[CMD] nvcc version: nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2025 NVIDIA Corporation
Built on Fri_Feb_21_20:23:50_PST_2025
Cuda compilation tools, release 12.8, V12.8.93
Build cuda_12.8.r12.8/compiler.35583870_0


===============================================================================
  PHASE 3: DEEP GPU ANALYSIS
  2025-12-18 20:59:48
===============================================================================


--- Full GPU Query ---

[CMD] nvidia-smi full query: nvidia-smi -q

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:48 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    Product Name                          : NVIDIA B200
    Product Brand                         : NVIDIA
    Product Architecture                  : Blackwell
    Display Mode                          : Disabled
    Display Active                        : Disabled
    Persistence Mode                      : Enabled
    Addressing Mode                       : HMM
    MIG Mode
        Current                           : Disabled
        Pending                           : Disabled
    Accounting Mode                       : Disabled
    Accounting Mode Buffer Size           : 4000
    Driver Model
        Current                           : N/A
        Pending                           : N/A
    Serial Number                         : 1655124005020
    GPU UUID                              : GPU-8741c41e-32f5-8eb1-fa98-6ab21942fa2f
    Minor Number                          : 0
    VBIOS Version                         : 97.00.9A.00.0F
    MultiGPU Board                        : No
    Board ID                              : 0x600
    Board Part Number                     : 692-2G525-0220-000
    GPU Part Number                       : 2901-886-A1
    FRU Part Number                       : N/A
    Platform Info
        Chassis Serial Number             : 
        Slot Number                       : N/A
        Tray Index                        : N/A
        Host ID                           : 1
        Peer Type                         : Switch Connected
        Module Id                         : 5
        GPU Fabric GUID                   : 0x8affabfaf6e841bf
    Inforom Version
        Image Version                     : G525.0220.00.03
        OEM Object                        : 2.1
        ECC Object                        : 7.16
        Power Management Object           : N/A
    Inforom BBX Object Flush
        Latest Timestamp                  : N/A
        Latest Duration                   : N/A
    GPU Operation Mode
        Current                           : N/A
        Pending                           : N/A
    GPU C2C Mode                          : Disabled
    GPU Virtualization Mode
        Virtualization Mode               : Pass-Through
        Host VGPU Mode                    : N/A
        vGPU Heterogeneous Mode           : N/A
    GPU Reset Status
        Reset Required                    : Requested functionality has been deprecated
        Drain and Reset Recommended       : Requested functionality has been deprecated
    GPU Recovery Action                   : None
    GSP Firmware Version                  : 570.148.08
    IBMNPU
        Relaxed Ordering Mode             : N/A
    PCI
        Bus                               : 0x06
        Device                            : 0x00
        Domain                            : 0x0000
        Base Classcode                    : 0x3
        Sub Classcode                     : 0x2
        Device Id                         : 0x290110DE
        Bus Id                            : 00000000:06:00.0
        Sub System Id                     : 0x199910DE
        GPU Link Info
            PCIe Generation
                Max                       : 5
                Current                   : 5
                Device Current            : 5
                Device Max                : 5
                Host Max                  : N/A
            Link Width
                Max                       : 16x
                Current                   : 16x
        Bridge Chip
            Type                          : N/A
            Firmware                      : N/A
        Replays Since Reset               : 0
        Replay Number Rollovers           : 0
        Tx Throughput                     : 1007 KB/s
        Rx Throughput                     : 752 KB/s
        Atomic Caps Outbound              : FETCHADD_32 FETCHADD_64 SWAP_32 SWAP_64 CAS_32 CAS_64 
        Atomic Caps Inbound               : FETCHADD_32 FETCHADD_64 SWAP_32 SWAP_64 CAS_32 CAS_64 
    Fan Speed                             : N/A
    Performance State                     : P0
    Clocks Event Reasons
        Idle                              : Active
        Applications Clocks Setting       : Not Active
        SW Power Cap                      : Not Active
        HW Slowdown                       : Not Active
            HW Thermal Slowdown           : Not Active
            HW Power Brake Slowdown       : Not Active
        Sync Boost                        : Not Active
        SW Thermal Slowdown               : Not Active
        Display Clock Setting             : Not Active
    Sparse Operation Mode                 : N/A
    FB Memory Usage
        Total                             : 183359 MiB
        Reserved                          : 717 MiB
        Used                              : 0 MiB
        Free                              : 182643 MiB
    BAR1 Memory Usage
        Total                             : 262144 MiB
        Used                              : 1 MiB
        Free                              : 262143 MiB
    Conf Compute Protected Memory Usage
        Total                             : 0 MiB
        Used                              : 0 MiB
        Free                              : 0 MiB
    Compute Mode                          : Default
    Utilization
        GPU                               : 0 %
        Memory                            : 0 %
        Encoder                           : 0 %
        Decoder                           : 0 %
        JPEG                              : 0 %
        OFA                               : 0 %
    Encoder Stats
        Active Sessions                   : 0
        Average FPS                       : 0
        Average Latency                   : 0
    FBC Stats
        Active Sessions                   : 0
        Average FPS                       : 0
        Average Latency                   : 0
    DRAM Encryption Mode
        Current                           : N/A
        Pending                           : N/A
    ECC Mode
        Current                           : Enabled
        Pending                           : Enabled
    ECC Errors
        Volatile
            SRAM Correctable              : 0
            SRAM Uncorrectable Parity     : 0
            SRAM Uncorrectable SEC-DED    : 0
            DRAM Correctable              : 0
            DRAM Uncorrectable            : 0
        Aggregate
            SRAM Correctable              : 0
            SRAM Uncorrectable Parity     : 0
            SRAM Uncorrectable SEC-DED    : 0
            DRAM Correctable              : 0
            DRAM Uncorrectable            : 0
            SRAM Threshold Exceeded       : No
        Aggregate Uncorrectable SRAM Sources
            SRAM L2                       : 0
            SRAM SM                       : 0
            SRAM Microcontroller          : 0
            SRAM PCIE                     : 0
            SRAM Other                    : 0
    Retired Pages
        Single Bit ECC                    : N/A
        Double Bit ECC                    : N/A
        Pending Page Blacklist            : N/A
    Remapped Rows
        Correctable Error                 : 0
        Uncorrectable Error               : 0
        Pending                           : No
        Remapping Failure Occurred        : No
        Bank Remap Availability Histogram
            Max                           : 3840 bank(s)
            High                          : 0 bank(s)
            Partial                       : 0 bank(s)
            Low                           : 0 bank(s)
            None                          : 0 bank(s)
    Temperature
        GPU Current Temp                  : 33 C
        GPU T.Limit Temp                  : 55 C
        GPU Shutdown T.Limit Temp         : -5 C
        GPU Slowdown T.Limit Temp         : -3 C
        GPU Max Operating T.Limit Temp    : 0 C
        GPU Target Temperature            : N/A
        Memory Current Temp               : 33 C
        Memory Max Operating T.Limit Temp : 0 C
    GPU Power Readings
        Average Power Draw                : 143.33 W
        Instantaneous Power Draw          : 141.76 W
        Current Power Limit               : 1000.00 W
        Requested Power Limit             : 1000.00 W
        Default Power Limit               : 1000.00 W
        Min Power Limit                   : 200.00 W
        Max Power Limit                   : 1000.00 W
    GPU Memory Power Readings 
        Average Power Draw                : 22.79 W
        Instantaneous Power Draw          : N/A
    Module Power Readings
        Average Power Draw                : N/A
        Instantaneous Power Draw          : N/A
        Current Power Limit               : N/A
        Requested Power Limit             : N/A
        Default Power Limit               : N/A
        Min Power Limit                   : N/A
        Max Power Limit                   : N/A
    Power Smoothing                       : Insufficient Permissions
    Workload Power Profiles
        Requested Profiles                : N/A
        Enforced Profiles                 : N/A
    Clocks
        Graphics                          : 120 MHz
        SM                                : 120 MHz
        Memory                            : 3996 MHz
        Video                             : 600 MHz
    Applications Clocks
        Graphics                          : 1965 MHz
        Memory                            : 3996 MHz
    Default Applications Clocks
        Graphics                          : 1965 MHz
        Memory                            : 3996 MHz
    Deferred Clocks
        Memory                            : N/A
    Max Clocks
        Graphics                          : 1965 MHz
        SM                                : 1965 MHz
        Memory                            : 3996 MHz
        Video                             : 1965 MHz
    Max Customer Boost Clocks
        Graphics                          : 1965 MHz
    Clock Policy
        Auto Boost                        : N/A
        Auto Boost Default                : N/A
    Voltage
        Graphics                          : N/A
    Fabric
        State                             : Completed
        Status                            : Success
        CliqueId                          : 0
        ClusterUUID                       : 00000000-0000-0000-0000-000000000000
        Health
            Bandwidth                     : N/A
            Route Recovery in progress    : N/A
            Route Unhealthy               : N/A
            Access Timeout Recovery       : False
    Processes                             : None
    Capabilities
        EGM                               : disabled



--- GPU Topology (NVLink 5.0 / NVSwitch 4.0) ---

[INFO] B200 NVLink 5.0: 18 links per GPU, 50GB/s per link, 1.8TB/s total
[INFO] HGX B200: 2 NVSwitch chips + 8 GPUs, 9 NVLinks per GPU to each switch

[CMD] Topology matrix: nvidia-smi topo -m
	[4mGPU0	NIC0	CPU Affinity	NUMA Affinity	GPU NUMA ID[0m
GPU0	 X 	PHB	0-25	0		N/A
NIC0	PHB	 X 				

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks

NIC Legend:

  NIC0: mlx5_0


[CMD] Topology with PCIe: nvidia-smi topo -mp
	[4mGPU0	NIC0	CPU Affinity	NUMA Affinity	GPU NUMA ID[0m
GPU0	 X 	PHB	0-25	0		N/A
NIC0	PHB	 X 				

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge

NIC Legend:

  NIC0: mlx5_0



--- NVLink Status (Critical for B200 Performance) ---

[CMD] NVLink status: nvidia-smi nvlink -s
GPU 0: NVIDIA B200 (UUID: GPU-8741c41e-32f5-8eb1-fa98-6ab21942fa2f)
	 Link 0: 50 GB/s
	 Link 1: 50 GB/s
	 Link 2: 50 GB/s
	 Link 3: 50 GB/s
	 Link 4: 50 GB/s
	 Link 5: 50 GB/s
	 Link 6: 50 GB/s
	 Link 7: 50 GB/s
	 Link 8: 50 GB/s
	 Link 9: 50 GB/s
	 Link 10: 50 GB/s
	 Link 11: 50 GB/s
	 Link 12: 50 GB/s
	 Link 13: 50 GB/s
	 Link 14: 50 GB/s
	 Link 15: 50 GB/s
	 Link 16: 50 GB/s
	 Link 17: 50 GB/s

[CMD] NVLink capabilities: nvidia-smi nvlink -c
GPU 0: NVIDIA B200 (UUID: GPU-8741c41e-32f5-8eb1-fa98-6ab21942fa2f)
	 Link 0, P2P is supported: true
	 Link 0, Access to system memory supported: true
	 Link 0, P2P atomics supported: true
	 Link 0, System memory atomics supported: true
	 Link 0, SLI is supported: true
	 Link 0, Link is supported: true
	 Link 1, P2P is supported: true
	 Link 1, Access to system memory supported: true
	 Link 1, P2P atomics supported: true
	 Link 1, System memory atomics supported: true
	 Link 1, SLI is supported: true
	 Link 1, Link is supported: true
	 Link 2, P2P is supported: true
	 Link 2, Access to system memory supported: true
	 Link 2, P2P atomics supported: true
	 Link 2, System memory atomics supported: true
	 Link 2, SLI is supported: true
	 Link 2, Link is supported: true
	 Link 3, P2P is supported: true
	 Link 3, Access to system memory supported: true
	 Link 3, P2P atomics supported: true
	 Link 3, System memory atomics supported: true
	 Link 3, SLI is supported: true
	 Link 3, Link is supported: true
	 Link 4, P2P is supported: true
	 Link 4, Access to system memory supported: true
	 Link 4, P2P atomics supported: true
	 Link 4, System memory atomics supported: true
	 Link 4, SLI is supported: true
	 Link 4, Link is supported: true
	 Link 5, P2P is supported: true
	 Link 5, Access to system memory supported: true
	 Link 5, P2P atomics supported: true
	 Link 5, System memory atomics supported: true
	 Link 5, SLI is supported: true
	 Link 5, Link is supported: true
	 Link 6, P2P is supported: true
	 Link 6, Access to system memory supported: true
	 Link 6, P2P atomics supported: true
	 Link 6, System memory atomics supported: true
	 Link 6, SLI is supported: true
	 Link 6, Link is supported: true
	 Link 7, P2P is supported: true
	 Link 7, Access to system memory supported: true
	 Link 7, P2P atomics supported: true
	 Link 7, System memory atomics supported: true
	 Link 7, SLI is supported: true
	 Link 7, Link is supported: true
	 Link 8, P2P is supported: true
	 Link 8, Access to system memory supported: true
	 Link 8, P2P atomics supported: true
	 Link 8, System memory atomics supported: true
	 Link 8, SLI is supported: true
	 Link 8, Link is supported: true
	 Link 9, P2P is supported: true
	 Link 9, Access to system memory supported: true
	 Link 9, P2P atomics supported: true
	 Link 9, System memory atomics supported: true
	 Link 9, SLI is supported: true
	 Link 9, Link is supported: true
	 Link 10, P2P is supported: true
	 Link 10, Access to system memory supported: true
	 Link 10, P2P atomics supported: true
	 Link 10, System memory atomics supported: true
	 Link 10, SLI is supported: true
	 Link 10, Link is supported: true
	 Link 11, P2P is supported: true
	 Link 11, Access to system memory supported: true
	 Link 11, P2P atomics supported: true
	 Link 11, System memory atomics supported: true
	 Link 11, SLI is supported: true
	 Link 11, Link is supported: true
	 Link 12, P2P is supported: true
	 Link 12, Access to system memory supported: true
	 Link 12, P2P atomics supported: true
	 Link 12, System memory atomics supported: true
	 Link 12, SLI is supported: true
	 Link 12, Link is supported: true
	 Link 13, P2P is supported: true
	 Link 13, Access to system memory supported: true
	 Link 13, P2P atomics supported: true
	 Link 13, System memory atomics supported: true
	 Link 13, SLI is supported: true
	 Link 13, Link is supported: true
	 Link 14, P2P is supported: true
	 Link 14, Access to system memory supported: true
	 Link 14, P2P atomics supported: true
	 Link 14, System memory atomics supported: true
	 Link 14, SLI is supported: true
	 Link 14, Link is supported: true
	 Link 15, P2P is supported: true
	 Link 15, Access to system memory supported: true
	 Link 15, P2P atomics supported: true
	 Link 15, System memory atomics supported: true
	 Link 15, SLI is supported: true
	 Link 15, Link is supported: true
	 Link 16, P2P is supported: true
	 Link 16, Access to system memory supported: true
	 Link 16, P2P atomics supported: true
	 Link 16, System memory atomics supported: true
	 Link 16, SLI is supported: true
	 Link 16, Link is supported: true
	 Link 17, P2P is supported: true
	 Link 17, Access to system memory supported: true
	 Link 17, P2P atomics supported: true
	 Link 17, System memory atomics supported: true
	 Link 17, SLI is supported: true
	 Link 17, Link is supported: true

[CMD] NVLink error counters: nvidia-smi nvlink -e
GPU 0: NVIDIA B200 (UUID: GPU-8741c41e-32f5-8eb1-fa98-6ab21942fa2f)
	 Link 0: Tx packets: 136
	 Link 0: Tx bytes: 39168
	 Link 0: Rx packets: 136
	 Link 0: Rx bytes: 39168
	 Link 0: Malformed packet Errors: 0
	 Link 0: Buffer overrun Errors: 0
	 Link 0: Rx Errors: 0
	 Link 0: Rx remote Errors: 0
	 Link 0: Rx General Errors: 0
	 Link 0: Local link integrity Errors: 0
	 Link 0: Tx discards: 0
	 Link 0: Link recovery successful events: 0
	 Link 0: Link recovery failed events: 0
	 Link 0: Total link recovery events: 0
	 Link 0: Effective Errors: 0
	 Link 0: Effective BER: 15e-255
	 Link 0: Symbol Errors: 0
	 Link 0: Symbol BER: 15e-255
	 Link 0: FEC Errors - 0: 2287698802
	 Link 0: FEC Errors - 1: 2
	 Link 0: FEC Errors - 2: 0
	 Link 0: FEC Errors - 3: 0
	 Link 0: FEC Errors - 4: 0
	 Link 0: FEC Errors - 5: 0
	 Link 0: FEC Errors - 6: 0
	 Link 0: FEC Errors - 7: 0
	 Link 0: FEC Errors - 8: 0
	 Link 0: FEC Errors - 9: 0
	 Link 0: FEC Errors - 10: 0
	 Link 0: FEC Errors - 11: 0
	 Link 0: FEC Errors - 12: 0
	 Link 0: FEC Errors - 13: 0
	 Link 0: FEC Errors - 14: 0
	 Link 0: FEC Errors - 15: 0

	 Link 1: Tx packets: 84
	 Link 1: Tx bytes: 24192
	 Link 1: Rx packets: 84
	 Link 1: Rx bytes: 24192
	 Link 1: Malformed packet Errors: 0
	 Link 1: Buffer overrun Errors: 0
	 Link 1: Rx Errors: 0
	 Link 1: Rx remote Errors: 0
	 Link 1: Rx General Errors: 0
	 Link 1: Local link integrity Errors: 0
	 Link 1: Tx discards: 0
	 Link 1: Link recovery successful events: 0
	 Link 1: Link recovery failed events: 0
	 Link 1: Total link recovery events: 0
	 Link 1: Effective Errors: 0
	 Link 1: Effective BER: 15e-255
	 Link 1: Symbol Errors: 0
	 Link 1: Symbol BER: 15e-255
	 Link 1: FEC Errors - 0: 2257706442
	 Link 1: FEC Errors - 1: 2
	 Link 1: FEC Errors - 2: 0
	 Link 1: FEC Errors - 3: 0
	 Link 1: FEC Errors - 4: 0
	 Link 1: FEC Errors - 5: 0
	 Link 1: FEC Errors - 6: 0
	 Link 1: FEC Errors - 7: 0
	 Link 1: FEC Errors - 8: 0
	 Link 1: FEC Errors - 9: 0
	 Link 1: FEC Errors - 10: 0
	 Link 1: FEC Errors - 11: 0
	 Link 1: FEC Errors - 12: 0
	 Link 1: FEC Errors - 13: 0
	 Link 1: FEC Errors - 14: 0
	 Link 1: FEC Errors - 15: 0

	 Link 2: Tx packets: 76
	 Link 2: Tx bytes: 21888
	 Link 2: Rx packets: 76
	 Link 2: Rx bytes: 21888
	 Link 2: Malformed packet Errors: 0
	 Link 2: Buffer overrun Errors: 0
	 Link 2: Rx Errors: 0
	 Link 2: Rx remote Errors: 0
	 Link 2: Rx General Errors: 0
	 Link 2: Local link integrity Errors: 0
	 Link 2: Tx discards: 0
	 Link 2: Link recovery successful events: 0
	 Link 2: Link recovery failed events: 0
	 Link 2: Total link recovery events: 0
	 Link 2: Effective Errors: 0
	 Link 2: Effective BER: 15e-255
	 Link 2: Symbol Errors: 0
	 Link 2: Symbol BER: 15e-255
	 Link 2: FEC Errors - 0: 2270151200
	 Link 2: FEC Errors - 1: 0
	 Link 2: FEC Errors - 2: 0
	 Link 2: FEC Errors - 3: 0
	 Link 2: FEC Errors - 4: 0
	 Link 2: FEC Errors - 5: 0
	 Link 2: FEC Errors - 6: 0
	 Link 2: FEC Errors - 7: 0
	 Link 2: FEC Errors - 8: 0
	 Link 2: FEC Errors - 9: 0
	 Link 2: FEC Errors - 10: 0
	 Link 2: FEC Errors - 11: 0
	 Link 2: FEC Errors - 12: 0
	 Link 2: FEC Errors - 13: 0
	 Link 2: FEC Errors - 14: 0
	 Link 2: FEC Errors - 15: 0

	 Link 3: Tx packets: 82
	 Link 3: Tx bytes: 23616
	 Link 3: Rx packets: 82
	 Link 3: Rx bytes: 23616
	 Link 3: Malformed packet Errors: 0
	 Link 3: Buffer overrun Errors: 0
	 Link 3: Rx Errors: 0
	 Link 3: Rx remote Errors: 0
	 Link 3: Rx General Errors: 0
	 Link 3: Local link integrity Errors: 0
	 Link 3: Tx discards: 0
	 Link 3: Link recovery successful events: 0
	 Link 3: Link recovery failed events: 0
	 Link 3: Total link recovery events: 0
	 Link 3: Effective Errors: 0
	 Link 3: Effective BER: 15e-255
	 Link 3: Symbol Errors: 0
	 Link 3: Symbol BER: 15e-255
	 Link 3: FEC Errors - 0: 2268922692
	 Link 3: FEC Errors - 1: 0
	 Link 3: FEC Errors - 2: 0
	 Link 3: FEC Errors - 3: 0
	 Link 3: FEC Errors - 4: 0
	 Link 3: FEC Errors - 5: 0
	 Link 3: FEC Errors - 6: 0
	 Link 3: FEC Errors - 7: 0
	 Link 3: FEC Errors - 8: 0
	 Link 3: FEC Errors - 9: 0
	 Link 3: FEC Errors - 10: 0
	 Link 3: FEC Errors - 11: 0
	 Link 3: FEC Errors - 12: 0
	 Link 3: FEC Errors - 13: 0
	 Link 3: FEC Errors - 14: 0
	 Link 3: FEC Errors - 15: 0

	 Link 4: Tx packets: 74
	 Link 4: Tx bytes: 21312
	 Link 4: Rx packets: 74
	 Link 4: Rx bytes: 21312
	 Link 4: Malformed packet Errors: 0
	 Link 4: Buffer overrun Errors: 0
	 Link 4: Rx Errors: 0
	 Link 4: Rx remote Errors: 0
	 Link 4: Rx General Errors: 0
	 Link 4: Local link integrity Errors: 0
	 Link 4: Tx discards: 0
	 Link 4: Link recovery successful events: 0
	 Link 4: Link recovery failed events: 0
	 Link 4: Total link recovery events: 0
	 Link 4: Effective Errors: 0
	 Link 4: Effective BER: 15e-255
	 Link 4: Symbol Errors: 0
	 Link 4: Symbol BER: 15e-255
	 Link 4: FEC Errors - 0: 2265032552
	 Link 4: FEC Errors - 1: 0
	 Link 4: FEC Errors - 2: 0
	 Link 4: FEC Errors - 3: 0
	 Link 4: FEC Errors - 4: 0
	 Link 4: FEC Errors - 5: 0
	 Link 4: FEC Errors - 6: 0
	 Link 4: FEC Errors - 7: 0
	 Link 4: FEC Errors - 8: 0
	 Link 4: FEC Errors - 9: 0
	 Link 4: FEC Errors - 10: 0
	 Link 4: FEC Errors - 11: 0
	 Link 4: FEC Errors - 12: 0
	 Link 4: FEC Errors - 13: 0
	 Link 4: FEC Errors - 14: 0
	 Link 4: FEC Errors - 15: 0

	 Link 5: Tx packets: 76
	 Link 5: Tx bytes: 21888
	 Link 5: Rx packets: 76
	 Link 5: Rx bytes: 21888
	 Link 5: Malformed packet Errors: 0
	 Link 5: Buffer overrun Errors: 0
	 Link 5: Rx Errors: 0
	 Link 5: Rx remote Errors: 0
	 Link 5: Rx General Errors: 0
	 Link 5: Local link integrity Errors: 0
	 Link 5: Tx discards: 0
	 Link 5: Link recovery successful events: 0
	 Link 5: Link recovery failed events: 0
	 Link 5: Total link recovery events: 0
	 Link 5: Effective Errors: 0
	 Link 5: Effective BER: 15e-255
	 Link 5: Symbol Errors: 0
	 Link 5: Symbol BER: 15e-255
	 Link 5: FEC Errors - 0: 2271725448
	 Link 5: FEC Errors - 1: 0
	 Link 5: FEC Errors - 2: 0
	 Link 5: FEC Errors - 3: 0
	 Link 5: FEC Errors - 4: 0
	 Link 5: FEC Errors - 5: 0
	 Link 5: FEC Errors - 6: 0
	 Link 5: FEC Errors - 7: 0
	 Link 5: FEC Errors - 8: 0
	 Link 5: FEC Errors - 9: 0
	 Link 5: FEC Errors - 10: 0
	 Link 5: FEC Errors - 11: 0
	 Link 5: FEC Errors - 12: 0
	 Link 5: FEC Errors - 13: 0
	 Link 5: FEC Errors - 14: 0
	 Link 5: FEC Errors - 15: 0

	 Link 6: Tx packets: 74
	 Link 6: Tx bytes: 21312
	 Link 6: Rx packets: 74
	 Link 6: Rx bytes: 21312
	 Link 6: Malformed packet Errors: 0
	 Link 6: Buffer overrun Errors: 0
	 Link 6: Rx Errors: 0
	 Link 6: Rx remote Errors: 0
	 Link 6: Rx General Errors: 0
	 Link 6: Local link integrity Errors: 0
	 Link 6: Tx discards: 0
	 Link 6: Link recovery successful events: 0
	 Link 6: Link recovery failed events: 0
	 Link 6: Total link recovery events: 0
	 Link 6: Effective Errors: 0
	 Link 6: Effective BER: 15e-255
	 Link 6: Symbol Errors: 0
	 Link 6: Symbol BER: 15e-255
	 Link 6: FEC Errors - 0: 2267831596
	 Link 6: FEC Errors - 1: 12
	 Link 6: FEC Errors - 2: 0
	 Link 6: FEC Errors - 3: 0
	 Link 6: FEC Errors - 4: 0
	 Link 6: FEC Errors - 5: 0
	 Link 6: FEC Errors - 6: 0
	 Link 6: FEC Errors - 7: 0
	 Link 6: FEC Errors - 8: 0
	 Link 6: FEC Errors - 9: 0
	 Link 6: FEC Errors - 10: 0
	 Link 6: FEC Errors - 11: 0
	 Link 6: FEC Errors - 12: 0
	 Link 6: FEC Errors - 13: 0
	 Link 6: FEC Errors - 14: 0
	 Link 6: FEC Errors - 15: 0

	 Link 7: Tx packets: 76
	 Link 7: Tx bytes: 21888
	 Link 7: Rx packets: 76
	 Link 7: Rx bytes: 21888
	 Link 7: Malformed packet Errors: 0
	 Link 7: Buffer overrun Errors: 0
	 Link 7: Rx Errors: 0
	 Link 7: Rx remote Errors: 0
	 Link 7: Rx General Errors: 0
	 Link 7: Local link integrity Errors: 0
	 Link 7: Tx discards: 0
	 Link 7: Link recovery successful events: 0
	 Link 7: Link recovery failed events: 0
	 Link 7: Total link recovery events: 0
	 Link 7: Effective Errors: 0
	 Link 7: Effective BER: 15e-255
	 Link 7: Symbol Errors: 0
	 Link 7: Symbol BER: 15e-255
	 Link 7: FEC Errors - 0: 2276880666
	 Link 7: FEC Errors - 1: 0
	 Link 7: FEC Errors - 2: 0
	 Link 7: FEC Errors - 3: 0
	 Link 7: FEC Errors - 4: 0
	 Link 7: FEC Errors - 5: 0
	 Link 7: FEC Errors - 6: 0
	 Link 7: FEC Errors - 7: 0
	 Link 7: FEC Errors - 8: 0
	 Link 7: FEC Errors - 9: 0
	 Link 7: FEC Errors - 10: 0
	 Link 7: FEC Errors - 11: 0
	 Link 7: FEC Errors - 12: 0
	 Link 7: FEC Errors - 13: 0
	 Link 7: FEC Errors - 14: 0
	 Link 7: FEC Errors - 15: 0

	 Link 8: Tx packets: 76
	 Link 8: Tx bytes: 21888
	 Link 8: Rx packets: 76
	 Link 8: Rx bytes: 21888
	 Link 8: Malformed packet Errors: 0
	 Link 8: Buffer overrun Errors: 0
	 Link 8: Rx Errors: 0
	 Link 8: Rx remote Errors: 0
	 Link 8: Rx General Errors: 0
	 Link 8: Local link integrity Errors: 0
	 Link 8: Tx discards: 0
	 Link 8: Link recovery successful events: 0
	 Link 8: Link recovery failed events: 0
	 Link 8: Total link recovery events: 0
	 Link 8: Effective Errors: 0
	 Link 8: Effective BER: 15e-255
	 Link 8: Symbol Errors: 0
	 Link 8: Symbol BER: 15e-255
	 Link 8: FEC Errors - 0: 2271430862
	 Link 8: FEC Errors - 1: 0
	 Link 8: FEC Errors - 2: 0
	 Link 8: FEC Errors - 3: 0
	 Link 8: FEC Errors - 4: 0
	 Link 8: FEC Errors - 5: 0
	 Link 8: FEC Errors - 6: 0
	 Link 8: FEC Errors - 7: 0
	 Link 8: FEC Errors - 8: 0
	 Link 8: FEC Errors - 9: 0
	 Link 8: FEC Errors - 10: 0
	 Link 8: FEC Errors - 11: 0
	 Link 8: FEC Errors - 12: 0
	 Link 8: FEC Errors - 13: 0
	 Link 8: FEC Errors - 14: 0
	 Link 8: FEC Errors - 15: 0

	 Link 9: Tx packets: 82
	 Link 9: Tx bytes: 23616
	 Link 9: Rx packets: 82
	 Link 9: Rx bytes: 23616
	 Link 9: Malformed packet Errors: 0
	 Link 9: Buffer overrun Errors: 0
	 Link 9: Rx Errors: 0
	 Link 9: Rx remote Errors: 0
	 Link 9: Rx General Errors: 0
	 Link 9: Local link integrity Errors: 0
	 Link 9: Tx discards: 0
	 Link 9: Link recovery successful events: 0
	 Link 9: Link recovery failed events: 0
	 Link 9: Total link recovery events: 0
	 Link 9: Effective Errors: 0
	 Link 9: Effective BER: 15e-255
	 Link 9: Symbol Errors: 0
	 Link 9: Symbol BER: 15e-255
	 Link 9: FEC Errors - 0: 2269221726
	 Link 9: FEC Errors - 1: 0
	 Link 9: FEC Errors - 2: 0
	 Link 9: FEC Errors - 3: 0
	 Link 9: FEC Errors - 4: 0
	 Link 9: FEC Errors - 5: 0
	 Link 9: FEC Errors - 6: 0
	 Link 9: FEC Errors - 7: 0
	 Link 9: FEC Errors - 8: 0
	 Link 9: FEC Errors - 9: 0
	 Link 9: FEC Errors - 10: 0
	 Link 9: FEC Errors - 11: 0
	 Link 9: FEC Errors - 12: 0
	 Link 9: FEC Errors - 13: 0
	 Link 9: FEC Errors - 14: 0
	 Link 9: FEC Errors - 15: 0

	 Link 10: Tx packets: 76
	 Link 10: Tx bytes: 21888
	 Link 10: Rx packets: 76
	 Link 10: Rx bytes: 21888
	 Link 10: Malformed packet Errors: 0
	 Link 10: Buffer overrun Errors: 0
	 Link 10: Rx Errors: 0
	 Link 10: Rx remote Errors: 0
	 Link 10: Rx General Errors: 0
	 Link 10: Local link integrity Errors: 0
	 Link 10: Tx discards: 0
	 Link 10: Link recovery successful events: 0
	 Link 10: Link recovery failed events: 0
	 Link 10: Total link recovery events: 0
	 Link 10: Effective Errors: 0
	 Link 10: Effective BER: 15e-255
	 Link 10: Symbol Errors: 0
	 Link 10: Symbol BER: 15e-255
	 Link 10: FEC Errors - 0: 2271186790
	 Link 10: FEC Errors - 1: 0
	 Link 10: FEC Errors - 2: 0
	 Link 10: FEC Errors - 3: 0
	 Link 10: FEC Errors - 4: 0
	 Link 10: FEC Errors - 5: 0
	 Link 10: FEC Errors - 6: 0
	 Link 10: FEC Errors - 7: 0
	 Link 10: FEC Errors - 8: 0
	 Link 10: FEC Errors - 9: 0
	 Link 10: FEC Errors - 10: 0
	 Link 10: FEC Errors - 11: 0
	 Link 10: FEC Errors - 12: 0
	 Link 10: FEC Errors - 13: 0
	 Link 10: FEC Errors - 14: 0
	 Link 10: FEC Errors - 15: 0

	 Link 11: Tx packets: 82
	 Link 11: Tx bytes: 23616
	 Link 11: Rx packets: 82
	 Link 11: Rx bytes: 23616
	 Link 11: Malformed packet Errors: 0
	 Link 11: Buffer overrun Errors: 0
	 Link 11: Rx Errors: 0
	 Link 11: Rx remote Errors: 0
	 Link 11: Rx General Errors: 0
	 Link 11: Local link integrity Errors: 0
	 Link 11: Tx discards: 0
	 Link 11: Link recovery successful events: 0
	 Link 11: Link recovery failed events: 0
	 Link 11: Total link recovery events: 0
	 Link 11: Effective Errors: 0
	 Link 11: Effective BER: 15e-255
	 Link 11: Symbol Errors: 0
	 Link 11: Symbol BER: 15e-255
	 Link 11: FEC Errors - 0: 2269331520
	 Link 11: FEC Errors - 1: 0
	 Link 11: FEC Errors - 2: 0
	 Link 11: FEC Errors - 3: 0
	 Link 11: FEC Errors - 4: 0
	 Link 11: FEC Errors - 5: 0
	 Link 11: FEC Errors - 6: 0
	 Link 11: FEC Errors - 7: 0
	 Link 11: FEC Errors - 8: 0
	 Link 11: FEC Errors - 9: 0
	 Link 11: FEC Errors - 10: 0
	 Link 11: FEC Errors - 11: 0
	 Link 11: FEC Errors - 12: 0
	 Link 11: FEC Errors - 13: 0
	 Link 11: FEC Errors - 14: 0
	 Link 11: FEC Errors - 15: 0

	 Link 12: Tx packets: 76
	 Link 12: Tx bytes: 21888
	 Link 12: Rx packets: 76
	 Link 12: Rx bytes: 21888
	 Link 12: Malformed packet Errors: 0
	 Link 12: Buffer overrun Errors: 0
	 Link 12: Rx Errors: 0
	 Link 12: Rx remote Errors: 0
	 Link 12: Rx General Errors: 0
	 Link 12: Local link integrity Errors: 0
	 Link 12: Tx discards: 0
	 Link 12: Link recovery successful events: 0
	 Link 12: Link recovery failed events: 0
	 Link 12: Total link recovery events: 0
	 Link 12: Effective Errors: 0
	 Link 12: Effective BER: 15e-255
	 Link 12: Symbol Errors: 0
	 Link 12: Symbol BER: 15e-255
	 Link 12: FEC Errors - 0: 2273749474
	 Link 12: FEC Errors - 1: 0
	 Link 12: FEC Errors - 2: 0
	 Link 12: FEC Errors - 3: 0
	 Link 12: FEC Errors - 4: 0
	 Link 12: FEC Errors - 5: 0
	 Link 12: FEC Errors - 6: 0
	 Link 12: FEC Errors - 7: 0
	 Link 12: FEC Errors - 8: 0
	 Link 12: FEC Errors - 9: 0
	 Link 12: FEC Errors - 10: 0
	 Link 12: FEC Errors - 11: 0
	 Link 12: FEC Errors - 12: 0
	 Link 12: FEC Errors - 13: 0
	 Link 12: FEC Errors - 14: 0
	 Link 12: FEC Errors - 15: 0

	 Link 13: Tx packets: 76
	 Link 13: Tx bytes: 21888
	 Link 13: Rx packets: 76
	 Link 13: Rx bytes: 21888
	 Link 13: Malformed packet Errors: 0
	 Link 13: Buffer overrun Errors: 0
	 Link 13: Rx Errors: 0
	 Link 13: Rx remote Errors: 0
	 Link 13: Rx General Errors: 0
	 Link 13: Local link integrity Errors: 0
	 Link 13: Tx discards: 0
	 Link 13: Link recovery successful events: 0
	 Link 13: Link recovery failed events: 0
	 Link 13: Total link recovery events: 0
	 Link 13: Effective Errors: 0
	 Link 13: Effective BER: 15e-255
	 Link 13: Symbol Errors: 0
	 Link 13: Symbol BER: 15e-255
	 Link 13: FEC Errors - 0: 2276834012
	 Link 13: FEC Errors - 1: 0
	 Link 13: FEC Errors - 2: 0
	 Link 13: FEC Errors - 3: 0
	 Link 13: FEC Errors - 4: 0
	 Link 13: FEC Errors - 5: 0
	 Link 13: FEC Errors - 6: 0
	 Link 13: FEC Errors - 7: 0
	 Link 13: FEC Errors - 8: 0
	 Link 13: FEC Errors - 9: 0
	 Link 13: FEC Errors - 10: 0
	 Link 13: FEC Errors - 11: 0
	 Link 13: FEC Errors - 12: 0
	 Link 13: FEC Errors - 13: 0
	 Link 13: FEC Errors - 14: 0
	 Link 13: FEC Errors - 15: 0

	 Link 14: Tx packets: 74
	 Link 14: Tx bytes: 21312
	 Link 14: Rx packets: 74
	 Link 14: Rx bytes: 21312
	 Link 14: Malformed packet Errors: 0
	 Link 14: Buffer overrun Errors: 0
	 Link 14: Rx Errors: 0
	 Link 14: Rx remote Errors: 0
	 Link 14: Rx General Errors: 0
	 Link 14: Local link integrity Errors: 0
	 Link 14: Tx discards: 0
	 Link 14: Link recovery successful events: 0
	 Link 14: Link recovery failed events: 0
	 Link 14: Total link recovery events: 0
	 Link 14: Effective Errors: 0
	 Link 14: Effective BER: 15e-255
	 Link 14: Symbol Errors: 0
	 Link 14: Symbol BER: 15e-255
	 Link 14: FEC Errors - 0: 2268383784
	 Link 14: FEC Errors - 1: 0
	 Link 14: FEC Errors - 2: 0
	 Link 14: FEC Errors - 3: 0
	 Link 14: FEC Errors - 4: 0
	 Link 14: FEC Errors - 5: 0
	 Link 14: FEC Errors - 6: 0
	 Link 14: FEC Errors - 7: 0
	 Link 14: FEC Errors - 8: 0
	 Link 14: FEC Errors - 9: 0
	 Link 14: FEC Errors - 10: 0
	 Link 14: FEC Errors - 11: 0
	 Link 14: FEC Errors - 12: 0
	 Link 14: FEC Errors - 13: 0
	 Link 14: FEC Errors - 14: 0
	 Link 14: FEC Errors - 15: 0

	 Link 15: Tx packets: 92
	 Link 15: Tx bytes: 26496
	 Link 15: Rx packets: 92
	 Link 15: Rx bytes: 26496
	 Link 15: Malformed packet Errors: 0
	 Link 15: Buffer overrun Errors: 0
	 Link 15: Rx Errors: 0
	 Link 15: Rx remote Errors: 0
	 Link 15: Rx General Errors: 0
	 Link 15: Local link integrity Errors: 0
	 Link 15: Tx discards: 0
	 Link 15: Link recovery successful events: 0
	 Link 15: Link recovery failed events: 0
	 Link 15: Total link recovery events: 0
	 Link 15: Effective Errors: 0
	 Link 15: Effective BER: 15e-255
	 Link 15: Symbol Errors: 0
	 Link 15: Symbol BER: 15e-255
	 Link 15: FEC Errors - 0: 2274490536
	 Link 15: FEC Errors - 1: 0
	 Link 15: FEC Errors - 2: 0
	 Link 15: FEC Errors - 3: 0
	 Link 15: FEC Errors - 4: 0
	 Link 15: FEC Errors - 5: 0
	 Link 15: FEC Errors - 6: 0
	 Link 15: FEC Errors - 7: 0
	 Link 15: FEC Errors - 8: 0
	 Link 15: FEC Errors - 9: 0
	 Link 15: FEC Errors - 10: 0
	 Link 15: FEC Errors - 11: 0
	 Link 15: FEC Errors - 12: 0
	 Link 15: FEC Errors - 13: 0
	 Link 15: FEC Errors - 14: 0
	 Link 15: FEC Errors - 15: 0

	 Link 16: Tx packets: 74
	 Link 16: Tx bytes: 21312
	 Link 16: Rx packets: 74
	 Link 16: Rx bytes: 21312
	 Link 16: Malformed packet Errors: 0
	 Link 16: Buffer overrun Errors: 0
	 Link 16: Rx Errors: 0
	 Link 16: Rx remote Errors: 0
	 Link 16: Rx General Errors: 0
	 Link 16: Local link integrity Errors: 0
	 Link 16: Tx discards: 0
	 Link 16: Link recovery successful events: 0
	 Link 16: Link recovery failed events: 0
	 Link 16: Total link recovery events: 0
	 Link 16: Effective Errors: 0
	 Link 16: Effective BER: 15e-255
	 Link 16: Symbol Errors: 0
	 Link 16: Symbol BER: 15e-255
	 Link 16: FEC Errors - 0: 2267915080
	 Link 16: FEC Errors - 1: 0
	 Link 16: FEC Errors - 2: 0
	 Link 16: FEC Errors - 3: 0
	 Link 16: FEC Errors - 4: 0
	 Link 16: FEC Errors - 5: 0
	 Link 16: FEC Errors - 6: 0
	 Link 16: FEC Errors - 7: 0
	 Link 16: FEC Errors - 8: 0
	 Link 16: FEC Errors - 9: 0
	 Link 16: FEC Errors - 10: 0
	 Link 16: FEC Errors - 11: 0
	 Link 16: FEC Errors - 12: 0
	 Link 16: FEC Errors - 13: 0
	 Link 16: FEC Errors - 14: 0
	 Link 16: FEC Errors - 15: 0

	 Link 17: Tx packets: 105
	 Link 17: Tx bytes: 30240
	 Link 17: Rx packets: 103
	 Link 17: Rx bytes: 29664
	 Link 17: Malformed packet Errors: 0
	 Link 17: Buffer overrun Errors: 0
	 Link 17: Rx Errors: 0
	 Link 17: Rx remote Errors: 0
	 Link 17: Rx General Errors: 0
	 Link 17: Local link integrity Errors: 0
	 Link 17: Tx discards: 0
	 Link 17: Link recovery successful events: 0
	 Link 17: Link recovery failed events: 0
	 Link 17: Total link recovery events: 0
	 Link 17: Effective Errors: 0
	 Link 17: Effective BER: 15e-255
	 Link 17: Symbol Errors: 0
	 Link 17: Symbol BER: 15e-255
	 Link 17: FEC Errors - 0: 2284049504
	 Link 17: FEC Errors - 1: 0
	 Link 17: FEC Errors - 2: 0
	 Link 17: FEC Errors - 3: 0
	 Link 17: FEC Errors - 4: 0
	 Link 17: FEC Errors - 5: 0
	 Link 17: FEC Errors - 6: 0
	 Link 17: FEC Errors - 7: 0
	 Link 17: FEC Errors - 8: 0
	 Link 17: FEC Errors - 9: 0
	 Link 17: FEC Errors - 10: 0
	 Link 17: FEC Errors - 11: 0
	 Link 17: FEC Errors - 12: 0
	 Link 17: FEC Errors - 13: 0
	 Link 17: FEC Errors - 14: 0
	 Link 17: FEC Errors - 15: 0



--- GPU Clocks and Power ---

[CMD] Clock speeds: nvidia-smi -q -d CLOCK

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:49 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    Clocks
        Graphics                          : 742 MHz
        SM                                : 742 MHz
        Memory                            : 3996 MHz
        Video                             : 772 MHz
    Applications Clocks
        Graphics                          : 1965 MHz
        Memory                            : 3996 MHz
    Default Applications Clocks
        Graphics                          : 1965 MHz
        Memory                            : 3996 MHz
    Deferred Clocks
        Memory                            : N/A
    Max Clocks
        Graphics                          : 1965 MHz
        SM                                : 1965 MHz
        Memory                            : 3996 MHz
        Video                             : 1965 MHz
    Max Customer Boost Clocks
        Graphics                          : 1965 MHz
    SM Clock Samples
        Duration                          : N/A
        Number of Samples                 : N/A
        Max                               : N/A
        Min                               : N/A
        Avg                               : N/A
    Memory Clock Samples
        Duration                          : N/A
        Number of Samples                 : N/A
        Max                               : N/A
        Min                               : N/A
        Avg                               : N/A
    Clock Policy
        Auto Boost                        : N/A
        Auto Boost Default                : N/A


[CMD] Power readings: nvidia-smi -q -d POWER

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:49 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    GPU Power Readings
        Average Power Draw                : 144.24 W
        Instantaneous Power Draw          : 145.76 W
        Current Power Limit               : 1000.00 W
        Requested Power Limit             : 1000.00 W
        Default Power Limit               : 1000.00 W
        Min Power Limit                   : 200.00 W
        Max Power Limit                   : 1000.00 W
    Power Samples
        Duration                          : 2.42 sec
        Number of Samples                 : 119
        Max                               : 185.70 W
        Min                               : 140.44 W
        Avg                               : 142.83 W
    GPU Memory Power Readings 
        Average Power Draw                : 22.67 W
        Instantaneous Power Draw          : N/A
    Module Power Readings
        Average Power Draw                : N/A
        Instantaneous Power Draw          : N/A
        Current Power Limit               : N/A
        Requested Power Limit             : N/A
        Default Power Limit               : N/A
        Min Power Limit                   : N/A
        Max Power Limit                   : N/A


[CMD] Performance state: nvidia-smi -q -d PERFORMANCE

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:49 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    Performance State                     : P0
    Clocks Event Reasons
        Idle                              : Active
        Applications Clocks Setting       : Not Active
        SW Power Cap                      : Not Active
        HW Slowdown                       : Not Active
            HW Thermal Slowdown           : Not Active
            HW Power Brake Slowdown       : Not Active
        Sync Boost                        : Not Active
        SW Thermal Slowdown               : Not Active
        Display Clock Setting             : Not Active
    Sparse Operation Mode                 : N/A



--- GPU Memory (B200: 180GB HBM3e expected) ---

[CMD] Memory info: nvidia-smi -q -d MEMORY

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:49 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    FB Memory Usage
        Total                             : 183359 MiB
        Reserved                          : 717 MiB
        Used                              : 0 MiB
        Free                              : 182643 MiB
    BAR1 Memory Usage
        Total                             : 262144 MiB
        Used                              : 1 MiB
        Free                              : 262143 MiB
    Conf Compute Protected Memory Usage
        Total                             : 0 MiB
        Used                              : 0 MiB
        Free                              : 0 MiB


[CMD] ECC status: nvidia-smi -q -d ECC

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:49 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    ECC Mode
        Current                           : Enabled
        Pending                           : Enabled
    ECC Errors
        Volatile
            SRAM Correctable              : 0
            SRAM Uncorrectable Parity     : 0
            SRAM Uncorrectable SEC-DED    : 0
            DRAM Correctable              : 0
            DRAM Uncorrectable            : 0
        Aggregate
            SRAM Correctable              : 0
            SRAM Uncorrectable Parity     : 0
            SRAM Uncorrectable SEC-DED    : 0
            DRAM Correctable              : 0
            DRAM Uncorrectable            : 0
            SRAM Threshold Exceeded       : No
        Aggregate Uncorrectable SRAM Sources
            SRAM L2                       : 0
            SRAM SM                       : 0
            SRAM Microcontroller          : 0
            SRAM PCIE                     : 0
            SRAM Other                    : 0


[CMD] Retired pages: nvidia-smi -q -d PAGE_RETIREMENT

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:49 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    Retired Pages
        Single Bit ECC                    : N/A
        Double Bit ECC                    : N/A
        Pending Page Blacklist            : N/A



--- MIG Configuration ---

[CMD] MIG mode: nvidia-smi -q -d MIG
Failed to parse --display/-d flags
[WARN] Command returned non-zero exit code

[CMD] MIG devices: nvidia-smi mig -lgi
No MIG-enabled devices found.


--- Compute Mode and Processes ---

[CMD] Compute mode: nvidia-smi -q -d COMPUTE

==============NVSMI LOG==============

Timestamp                                 : Thu Dec 18 20:59:49 2025
Driver Version                            : 570.148.08
CUDA Version                              : 12.8

Attached GPUs                             : 1
GPU 00000000:06:00.0
    Compute Mode                          : Default


[CMD] Running processes: nvidia-smi pmon -c 1
# gpu         pid   type     sm    mem    enc    dec    jpg    ofa    command 
# Idx           #    C/G      %      %      %      %      %      %    name 
    0          -     -      -      -      -      -      -      -    -              


--- PCIe Configuration (B200: Gen5 x16 expected) ---

[CMD] GPU PCIe info: nvidia-smi -q -d PCIE
Failed to parse --display/-d flags
[WARN] Command returned non-zero exit code

[CMD] PCIe link gen/width: nvidia-smi --query-gpu=pcie.link.gen.current,pcie.link.width.current --format=csv
pcie.link.gen.current, pcie.link.width.current
5, 16


--- All GPU Properties ---

[CMD] GPU properties CSV: nvidia-smi --query-gpu=name,driver_version,pstate,memory.total,memory.used,memory.free,compute_mode,gpu_bus_id,gpu_uuid --format=csv
name, driver_version, pstate, memory.total [MiB], memory.used [MiB], memory.free [MiB], compute_mode, pci.bus_id, uuid
NVIDIA B200, 570.148.08, P0, 183359 MiB, 0 MiB, 182643 MiB, Default, 00000000:06:00.0, GPU-8741c41e-32f5-8eb1-fa98-6ab21942fa2f


--- DCGM (Data Center GPU Manager) ---

[SKIP] DCGM not installed (optional for monitoring)

===============================================================================
  PHASE 4: INFINIBAND / RDMA NETWORKING
  2025-12-18 20:59:49
===============================================================================


--- OFED Version (Lambda includes MLNX_OFED) ---

[INFO] Lambda Labs includes OFED for InfiniBand/RDMA support
[INFO] MLNX_OFED transitioning to DOCA-OFED (Jan 2025+)

[CMD] OFED version: ofed_info -s
OFED-internal-24.10-2.1.8:

[CMD] OFED full info: ofed_info
OFED-internal-24.10-2.1.8:

clusterkit:
/sw/release/mlnx_ofed/IBHPC/OFED-internal-24.10-1.1.4/SRPMS/clusterkit-1.14.462-1.2410068.src.rpm

dpcp:
/sw/release/mlnx_ofed/IBHPC/OFED-internal-24.10-1.1.4/SRPMS/dpcp-1.1.50-1.2410068.src.rpm

fwctl:
https://git-nbu.nvidia.com/r/a/mlnx_ofed/mlnx-ofa_kernel-4.0.git mlnx_ofed_24_10
commit 57699cda8ac0958195add960e12237251f5d04ab

hcoll:
/sw/release/mlnx_ofed/IBHPC/OFED-internal-24.10-1.1.4/SRPMS/hcoll-4.8.3230-1.2410068.src.rpm

ibarr:
/sw/release/mlnx_ofed/IBHPC/OFED-internal-24.10-1.1.4/SRPMS/ibarr-0.1.3-1.2410068.src.rpm

ibdump:
/sw/release/mlnx_ofed/IBHPC/OFED-internal-24.10-1.1.4/SRPMS/ibdump-6.0.0-1.2410068.src.rpm

ibsim:
/sw/release/mlnx_ofed/IBHPC/OFED-internal-24.10-1.1.4/SRPMS/ibsim-0.12-1.2410068.src.rpm

ibutils2:
/sw/release/mlnx_ofed/IBHPC/OFED-internal-24.10-1.1.4/SRPMS/ibutils2-2.1.1-0.21902.MLNX20241029.g46cf6278.2410068.src.rpm

iser:
https://git-nbu.nvidia.com/r/a/mlnx_ofed/mlnx-ofa_kernel-4.0.git mlnx_ofed_24_10
[CMD] DOCA version: doca_version
[SKIP] Command not found: doca_version


--- InfiniBand Device Status ---

[CMD] IB status: ibstat
CA 'mlx5_0'
	CA type: MT4126
	Number of ports: 1
	Firmware version: 28.40.1202
	Hardware version: 0
	Node GUID: 0x020017fffe02fba7
	System image GUID: 0xc470bd0300bae230
	Port 1:
		State: Active
		Physical state: LinkUp
		Rate: 200
		Base lid: 0
		LMC: 0
		SM lid: 0
		Capability mask: 0x00010000
		Port GUID: 0x000017fffe02fba7
		Link layer: Ethernet

[CMD] IB device info: ibv_devinfo
hca_id:	mlx5_0
	transport:			InfiniBand (0)
	fw_ver:				28.40.1202
	node_guid:			0200:17ff:fe02:fba7
	sys_image_guid:			c470:bd03:00ba:e230
	vendor_id:			0x02c9
	vendor_part_id:			4126
	hw_ver:				0x0
	board_id:			ORC0000000014
	phys_port_cnt:			1
		port:	1
			state:			PORT_ACTIVE (4)
			max_mtu:		4096 (5)
			active_mtu:		1024 (3)
			sm_lid:			0
			port_lid:		0
			port_lmc:		0x00
			link_layer:		Ethernet


[CMD] IB devices list: ibv_devices
    device          	   node GUID
    ------          	----------------
    mlx5_0          	020017fffe02fba7

[CMD] IB link info: iblinkinfo
ibwarn: [8612] mad_rpc_open_port: can't open UMAD port ((null):0)
Failed to open (null) port 0
[WARN] Command returned non-zero exit code


--- RDMA Configuration ---

[CMD] RDMA devices: rdma link show
link mlx5_0/1 state ACTIVE physical_state LINK_UP netdev eno1 

[CMD] RDMA system: rdma system show
netns shared copy-on-fork on

[DIR] /sys/class/infiniband:
total 0
drwxr-xr-x  2 root root 0 Dec 18 20:52 .
drwxr-xr-x 70 root root 0 Dec 18 20:52 ..
lrwxrwxrwx  1 root root 0 Dec 18 20:52 mlx5_0 -> ../../devices/pci0000:00/0000:00:02.4/0000:05:00.0/infiniband/mlx5_0


--- GPUDirect RDMA (nvidia-peermem) ---

[INFO] nvidia-peermem enables direct GPU-to-NIC transfers for InfiniBand
nvidia_peermem         16384  0
ib_uverbs             200704  3 nvidia_peermem,rdma_ucm,mlx5_ib
nvidia              11644928  8 nvidia_uvm,nvidia_peermem,nvidia_modeset
[CMD] Peer memory status: cat /sys/kernel/mm/memory_peers/nv_mem/version
cat: /sys/kernel/mm/memory_peers/nv_mem/version: No such file or directory
[WARN] Command returned non-zero exit code


--- Network Interfaces ---

[CMD] IP addresses: ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eno1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 02:00:17:02:fb:a7 brd ff:ff:ff:ff:ff:ff
    altname enp5s0
    inet 10.19.109.71/24 brd 10.19.109.255 scope global eno1
       valid_lft forever preferred_lft forever
    inet6 fe80::17ff:fe02:fba7/64 scope link 
       valid_lft forever preferred_lft forever
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default 
    link/ether 46:43:4a:2c:20:7c brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever

[CMD] Network interfaces: ip link show
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eno1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 02:00:17:02:fb:a7 brd ff:ff:ff:ff:ff:ff
    altname enp5s0
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
    link/ether 46:43:4a:2c:20:7c brd ff:ff:ff:ff:ff:ff

[CMD] Routing table: ip route
default via 10.19.109.1 dev eno1 proto static 
10.19.109.0/24 dev eno1 proto kernel scope link src 10.19.109.71 
172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown 

1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
2: eno1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
3: docker0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN mode DEFAULT group default 
	Speed: 200000Mb/s
	Duplex: Full
	Link detected: yes

===============================================================================
  PHASE 5: SYSTEM HARDWARE
  2025-12-18 20:59:49
===============================================================================


--- CPU Information ---

[CMD] lscpu: lscpu
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 57 bits virtual
Byte Order:                           Little Endian
CPU(s):                               26
On-line CPU(s) list:                  0-25
Vendor ID:                            GenuineIntel
Model name:                           INTEL(R) XEON(R) PLATINUM 8592+
CPU family:                           6
Model:                                207
Thread(s) per core:                   2
Core(s) per socket:                   13
Socket(s):                            1
Stepping:                             2
BogoMIPS:                             3800.00
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq dtes64 vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities
Virtualization:                       VT-x
Hypervisor vendor:                    KVM
Virtualization type:                  full
L1d cache:                            832 KiB (26 instances)
L1i cache:                            832 KiB (26 instances)
L2 cache:                             52 MiB (13 instances)
L3 cache:                             16 MiB (1 instance)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0-25
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Not affected
Vulnerability Mds:                    Not affected
Vulnerability Meltdown:               Not affected
Vulnerability Mmio stale data:        Not affected
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Not affected
Vulnerability Spec rstack overflow:   Not affected
Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Mitigation; TSX disabled

model name	: INTEL(R) XEON(R) PLATINUM 8592+

--- NUMA Topology (GPU-CPU affinity) ---

[CMD] numactl hardware: numactl --hardware
[SKIP] Command not found: numactl

[CMD] numactl show: numactl --show
[SKIP] Command not found: numactl

[CMD] NUMA info from lscpu: lscpu
NUMA node(s):                         1
NUMA node0 CPU(s):                    0-25

--- Memory Hardware ---

[CMD] dmidecode memory: sudo dmidecode -t memory
# dmidecode 3.3
Getting SMBIOS data from sysfs.
SMBIOS 3.0.0 present.

Handle 0x1000, DMI type 16, 23 bytes
Physical Memory Array
	Location: Other
	Use: System Memory
	Error Correction Type: Multi-bit ECC
	Maximum Capacity: 360 GB
	Error Information Handle: Not Provided
	Number Of Devices: 23

Handle 0x1100, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x1000
	Error Information Handle: Not Provided
	Total Width: Unknown
	Data Width: Unknown
	Size: 16 GB
	Form Factor: DIMM
	Set: None
	Locator: DIMM 0
	Bank Locator: Not Specified
	Type: RAM
	Type Detail: Other
	Speed: Unknown
	Manufacturer: QEMU
	Serial Number: Not Specified
	Asset Tag: Not Specified
	Part Number: Not Specified
	Rank: Unknown
	Configured Memory Speed: Unknown
	Minimum Voltage: Unknown
	Maximum Voltage: Unknown
	Configured Voltage: Unknown

Handle 0x1101, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x1000
	Error Information Handle: Not Provided
	Total Width: Unknown
	Data Width: Unknown
	Size: 16 GB
	Form Factor: DIMM
	Set: None
	Locator: DIMM 1
	Bank Locator: Not Specified
	Type: RAM
	Type Detail: Other
	Speed: Unknown
	Manufacturer: QEMU
	Serial Number: Not Specified
	Asset Tag: Not Specified
	Part Number: Not Specified
	Rank: Unknown
	Configured Memory Speed: Unknown
	Minimum Voltage: Unknown
	Maximum Voltage: Unknown
	Configured Voltage: Unknown

Handle 0x1102, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x1000
	Error Information Handle: Not Provided
	Total Width: Unknown
	Data Width: Unknown
	Size: 16 GB
	Form Factor: DIMM
	Set: None
	Locator: DIMM 2
	Bank Locator: Not Specified
	Type: RAM
	Type Detail: Other
	Speed: Unknown
	Manufacturer: QEMU
	Serial Number: Not Specified
	Asset Tag: Not Specified
	Part Number: Not Specified
	Rank: Unknown
	Configured Memory Speed: Unknown
	Minimum Voltage: Unknown
	Maximum Voltage: Unknown
	Configured Voltage: Unknown

Handle 0x1103, DMI type 17, 40 bytes
Memory Device
	Array Handle: 0x1000
	Error Information Handle: Not Provided
	Total Width: Unknown
	Data Width: Unknown
	Size: 16 GB
	Form Factor: DIMM
	Set: None
	Locator: DIMM 3
	Bank Locator: Not Specified
	Type: RAM
	Type Detail: Other
	Speed: Unknown
[SKIP] Need sudo for dmidecode
[FILE] /proc/meminfo:
MemTotal:       371266208 kB
MemFree:        367807308 kB
MemAvailable:   367507176 kB
Buffers:          248100 kB
Cached:          2155052 kB
SwapCached:            0 kB
Active:          1041008 kB
Inactive:        1574036 kB
Active(anon):     226468 kB
Inactive(anon):        0 kB
Active(file):     814540 kB
Inactive(file):  1574036 kB
Unevictable:       31616 kB
Mlocked:           27616 kB
SwapTotal:             0 kB
SwapFree:              0 kB
Zswap:                 0 kB
Zswapped:              0 kB
Dirty:               200 kB
Writeback:             0 kB
AnonPages:        243732 kB
Mapped:           188928 kB
Shmem:              5496 kB
KReclaimable:      72608 kB
Slab:             267500 kB
SReclaimable:      72608 kB
SUnreclaim:       194892 kB
KernelStack:        8032 kB
PageTables:         5332 kB
SecPageTables:         0 kB
NFS_Unstable:          0 kB
Bounce:                0 kB
WritebackTmp:          0 kB
CommitLimit:    185633104 kB
Committed_AS:    1163172 kB
VmallocTotal:   13743895347199 kB
VmallocUsed:       53708 kB
VmallocChunk:          0 kB
Percpu:            18032 kB
HardwareCorrupted:     0 kB
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
ShmemPmdMapped:        0 kB
FileHugePages:         0 kB
FilePmdMapped:         0 kB
Unaccepted:            0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB
DirectMap4k:      304804 kB
DirectMap2M:     3885056 kB
DirectMap1G:    375390208 kB


--- PCIe Devices ---

[CMD] All PCIe devices: lspci
00:00.0 Host bridge: Intel Corporation 82G33/G31/P35/P31 Express DRAM Controller
00:01.0 VGA compatible controller: Red Hat, Inc. Virtio GPU (rev 01)
00:02.0 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:02.1 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:02.2 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:02.3 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:02.4 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:02.5 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:02.6 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:02.7 PCI bridge: Red Hat, Inc. QEMU PCIe Root port
00:1f.0 ISA bridge: Intel Corporation 82801IB (ICH9) LPC Interface Controller (rev 02)
00:1f.2 SATA controller: Intel Corporation 82801IR/IO/IH (ICH9R/DO/DH) 6 port SATA Controller [AHCI mode] (rev 02)
00:1f.3 SMBus: Intel Corporation 82801I (ICH9 Family) SMBus Controller (rev 02)
01:00.0 Communication controller: Red Hat, Inc. Virtio console (rev 01)
02:00.0 USB controller: Red Hat, Inc. QEMU XHCI Host Controller (rev 01)
03:00.0 SCSI storage controller: Red Hat, Inc. Virtio block device (rev 01)
04:00.0 SCSI storage controller: Red Hat, Inc. Virtio block device (rev 01)
05:00.0 Ethernet controller: Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
06:00.0 3D controller: NVIDIA Corporation Device 2901 (rev a1)
07:00.0 Communication controller: Red Hat, Inc. Virtio socket (rev 01)

[CMD] NVIDIA PCIe detailed: lspci -d 10de: -vvv
06:00.0 3D controller: NVIDIA Corporation Device 2901 (rev a1)
	Subsystem: NVIDIA Corporation Device 1999
	Physical Slot: 0-6
	Control: I/O+ Mem+ BusMaster+ SpecCycle- MemWINV- VGASnoop- ParErr+ Stepping- SERR+ FastB2B- DisINTx+
	Status: Cap+ 66MHz- UDF- FastB2B- ParErr- DEVSEL=fast >TAbort- <TAbort- <MAbort- >SERR- <PERR- INTx-
	Latency: 0
	Interrupt: pin A routed to IRQ 22
	Region 0: Memory at 384000000000 (64-bit, prefetchable) [size=64M]
	Region 2: Memory at 380000000000 (64-bit, prefetchable) [size=256G]
	Region 4: Memory at 384004000000 (64-bit, prefetchable) [size=32M]
	Capabilities: <access denied>
	Kernel driver in use: nvidia
	Kernel modules: nvidiafb, nouveau, nvidia_drm, nvidia


[CMD] PCIe tree: lspci -tv
-[0000:00]-+-00.0  Intel Corporation 82G33/G31/P35/P31 Express DRAM Controller
           +-01.0  Red Hat, Inc. Virtio GPU
           +-02.0-[01]----00.0  Red Hat, Inc. Virtio console
           +-02.1-[02]----00.0  Red Hat, Inc. QEMU XHCI Host Controller
           +-02.2-[03]----00.0  Red Hat, Inc. Virtio block device
           +-02.3-[04]----00.0  Red Hat, Inc. Virtio block device
           +-02.4-[05]----00.0  Mellanox Technologies ConnectX Family mlx5Gen Virtual Function
           +-02.5-[06]----00.0  NVIDIA Corporation Device 2901
           +-02.6-[07]----00.0  Red Hat, Inc. Virtio socket
           +-02.7-[08]--
           +-1f.0  Intel Corporation 82801IB (ICH9) LPC Interface Controller
           +-1f.2  Intel Corporation 82801IR/IO/IH (ICH9R/DO/DH) 6 port SATA Controller [AHCI mode]
           \-1f.3  Intel Corporation 82801I (ICH9 Family) SMBus Controller


--- Storage ---

[CMD] Block devices: lsblk -o NAME,SIZE,TYPE,FSTYPE,MOUNTPOINT,MODEL
NAME      SIZE TYPE FSTYPE   MOUNTPOINT                 MODEL
loop0      55M loop squashfs /snap/aws-cli/1518         
loop1    63.8M loop squashfs /snap/core20/2599          
loop2    73.9M loop squashfs /snap/core22/2010          
loop3   429.2M loop squashfs /snap/google-cloud-sdk/575 
loop4    89.4M loop squashfs /snap/lxd/31333            
loop5    50.9M loop squashfs /snap/snapd/24718          
loop6    50.9M loop          /snap/snapd/25577          
loop7      74M loop          /snap/core22/2163          
loop8    91.4M loop squashfs /snap/lxd/36918            
loop9    52.9M loop squashfs /snap/aws-cli/1805         
vda       2.8T disk                                     
‚îú‚îÄvda1    2.7T part ext4     /                          
‚îú‚îÄvda14     4M part                                     
‚îî‚îÄvda15   106M part vfat     /boot/efi                  
vdb       420K disk iso9660                             

[CMD] Disk usage: df -h
Filesystem      Size  Used Avail Use% Mounted on
tmpfs            36G  1.3M   36G   1% /run
efivarfs        256K   20K  232K   8% /sys/firmware/efi/efivars
/dev/vda1       2.7T   26G  2.7T   1% /
tmpfs           178G  168K  178G   1% /dev/shm
tmpfs           5.0M     0  5.0M   0% /run/lock
/dev/vda15      105M  6.1M   99M   6% /boot/efi
tmpfs            36G   12K   36G   1% /run/user/1000

[CMD] NVMe devices: nvme list
[SKIP] Command not found: nvme


--- IOMMU Configuration ---

2
[CMD] Kernel cmdline: cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-6.8.0-60-generic root=UUID=9686db4d-2c4c-4029-a6ea-ca7b8bae111c ro intel_pstate=disable console=tty1 console=ttyS0


--- Huge Pages (GPU Memory Performance) ---

[INFO] Huge pages can improve GPU memory mapping performance
[CMD] Huge pages config: grep -i huge /proc/meminfo
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
FileHugePages:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:               0 kB

[CMD] Transparent huge pages: cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never

[CMD] THP defrag: cat /sys/kernel/mm/transparent_hugepage/defrag
always defer defer+madvise [madvise] never


--- Cgroups Configuration ---

[INFO] cgroups v2 is preferred for modern container runtimes
[CMD] Cgroups version: stat -fc %T /sys/fs/cgroup
cgroup2fs

cgroup2 on /sys/fs/cgroup type cgroup2 (rw,nosuid,nodev,noexec,relatime,nsdelegate,memory_recursiveprot)
[FILE] /sys/fs/cgroup/cgroup.controllers:
cpuset cpu io memory hugetlb pids rdma misc


===============================================================================
  PHASE 6: CUDA AND NVIDIA SOFTWARE STACK
  2025-12-18 20:59:49
===============================================================================


--- CUDA Installations ---

[CMD] CUDA version (nvcc): nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2025 NVIDIA Corporation
Built on Fri_Feb_21_20:23:50_PST_2025
Cuda compilation tools, release 12.8, V12.8.93
Build cuda_12.8.r12.8/compiler.35583870_0

[SKIP] File/dir not found: /usr/local/cuda/version.txt

[SKIP] File/dir not found: /usr/local/cuda/version.json

lrwxrwxrwx  1 root root   13 Jul  7 04:13 cuda -> /usr/lib/cuda

--- NVIDIA Libraries ---

	libnvidia-ptxjitcompiler.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so.1
	libnvidia-ptxjitcompiler.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-ptxjitcompiler.so
	libnvidia-pkcs11.so.570.148.08 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-pkcs11.so.570.148.08
	libnvidia-pkcs11-openssl3.so.570.148.08 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-pkcs11-openssl3.so.570.148.08
	libnvidia-opencl.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-opencl.so.1
	libnvidia-nvvm.so.4 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-nvvm.so.4
	libnvidia-nvvm.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-nvvm.so
	libnvidia-ml.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-ml.so.1
	libnvidia-ml.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-ml.so
	libnvidia-container.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-container.so.1
	libnvidia-container-go.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-container-go.so.1
	libnvidia-cfg.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-cfg.so.1
	libnvidia-cfg.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-cfg.so
	libnvidia-allocator.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-allocator.so.1
	libnvidia-allocator.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libnvidia-allocator.so
	libnccl.so.2 (libc6,x86-64) => /lib/x86_64-linux-gnu/libnccl.so.2
	libnccl.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libnccl.so
	libicudata.so.70 (libc6,x86-64) => /lib/x86_64-linux-gnu/libicudata.so.70
	libcudart.so.12 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcudart.so.12
	libcudart.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libcudart.so
	libcudadebugger.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcudadebugger.so.1
	libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1
	libcuda.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so
	libcublasLt.so.12 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcublasLt.so.12
	libcublasLt.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libcublasLt.so
	libcublas.so.12 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcublas.so.12
	libcublas.so (libc6,x86-64) => /lib/x86_64-linux-gnu/libcublas.so
[CMD] cuDNN version header: grep -E CUDNN_MAJOR|CUDNN_MINOR|CUDNN_PATCHLEVEL /usr/include/cudnn_version.h
grep: /usr/include/cudnn_version.h: No such file or directory
[WARN] Command returned non-zero exit code

[CMD] NCCL version header: grep -E NCCL_MAJOR|NCCL_MINOR|NCCL_PATCH /usr/include/nccl.h
#define NCCL_MAJOR 2
#define NCCL_MINOR 26
#define NCCL_PATCH 2
  NCCL_VERSION(NCCL_MAJOR, NCCL_MINOR, NCCL_PATCH), /* version */       \
  NCCL_VERSION(NCCL_MAJOR, NCCL_MINOR, NCCL_PATCH), /* version */           \


--- NVIDIA Packages ---

[CMD] NVIDIA dpkg packages: dpkg -l
ii  cudnn-license                                    9.8.0.87-0lambda1                            all          NVIDIA CUDA deep neural network, run-time libraries (license)
ii  ibarr:amd64                                      0.1.3-1.2410068                              amd64        Nvidia address and route userspace resolution services for Infiniband
ii  libaccinj64-12.8:amd64                           12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA ACCINJ Library (64-bit)
ii  libcu++-dev                                      2.7.0~12.8.1-0lambda0.22.04.1                all          NVIDIA C++ Standard Library
ii  libcublas12:amd64                                12.8.4.1~12.8.1-0lambda0.22.04.1             amd64        NVIDIA cuBLAS Library
ii  libcublaslt12:amd64                              12.8.4.1~12.8.1-0lambda0.22.04.1             amd64        NVIDIA cuBLASLt Library
ii  libcudart12:amd64                                12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Runtime Library
ii  libcufft11:amd64                                 11.3.3.83~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuFFT Library
ii  libcufftw11:amd64                                11.3.3.83~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuFFTW Library
ii  libcuinj64-12.8:amd64                            12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUINJ Library (64-bit)
ii  libcupti-dev:amd64                               12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Profiler Tools Interface development files
ii  libcupti12:amd64                                 12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Profiler Tools Interface runtime library
ii  libcurand10:amd64                                11.1.1+~10.3.9.90~12.8.1-0lambda0.22.04.1    amd64        NVIDIA cuRAND Library
ii  libcusolver11:amd64                              11.7.3.90~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuSOLVER Library
ii  libcusolvermg11:amd64                            11.7.3.90~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuSOLVERmg Library
ii  libcusparse12:amd64                              12.5.8.93~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuSPARSE Library
ii  libnccl-dev:amd64                                2.26.2-0lambda1                              amd64        NVIDIA Collectives Communication Library (NCCL) development files
ii  libnccl2:amd64                                   2.26.2-0lambda1                              amd64        NVIDIA Collectives Communication Library (NCCL) runtime
ii  libnppc12:amd64                                  12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives core runtime library
ii  libnppial12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Arithmetic and Logic
ii  libnppicc12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Color Conversion
ii  libnppidei12:amd64                               12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Data Exchange and Initialization
ii  libnppif12:amd64                                 12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Filters
ii  libnppig12:amd64                                 12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Geometry transforms
ii  libnppim12:amd64                                 12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Morphological operations
ii  libnppist12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Statistics
ii  libnppisu12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Support
ii  libnppitc12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Threshold and Compare
ii  libnpps12:amd64                                  12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives for signal processing runtime library
ii  libnvidia-cfg1-570-server:amd64                  570.148.08-0lambda0.22.04.1                  amd64        NVIDIA binary OpenGL/GLX configuration library
ii  libnvidia-compute-570-server:amd64               570.148.08-0lambda0.22.04.1                  amd64        NVIDIA libcompute package
ii  libnvidia-container-tools                        1.17.8+dfsg-0lambda0.22.04.1                 amd64        Package for configuring containers with NVIDIA hardware (CLI tool)
ii  libnvidia-container1:amd64                       1.17.8+dfsg-0lambda0.22.04.1                 amd64        Package for configuring containers with NVIDIA hardware (shared library)
ii  libnvidia-extra-570-server:amd64                 570.148.08-0lambda0.22.04.1                  amd64        Extra libraries for the NVIDIA driver
ii  libnvidia-ml-dev:amd64                           12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA Management Library (NVML) development files
ii  libnvjitlink12:amd64                             12.8.93~12.8.1-0lambda0.22.04.1              amd64        NVIDIA Compiler JIT LTO Library
ii  libnvjpeg12:amd64                                12.3.5.92~12.8.1-0lambda0.22.04.1            amd64        NVIDIA JPEG library (nvJPEG)
ii  libnvrtc-builtins12.8:amd64                      12.8.93~12.8.1-0lambda0.22.04.1              amd64        CUDA Runtime Compilation (NVIDIA NVRTC Builtins Library)
ii  libnvrtc12:amd64                                 12.8.93~12.8.1-0lambda0.22.04.1              amd64        CUDA Runtime Compilation (NVIDIA NVRTC Library)
[CMD] CUDA dpkg packages: dpkg -l
ii  cudnn-license                                    9.8.0.87-0lambda1                            all          NVIDIA CUDA deep neural network, run-time libraries (license)
ii  lambda-stack-cuda                                0.1.17~22.04.1                               all          Deep learning software stack from Lambda Labs (CUDA)
ii  libcub-dev                                       2.7.0~12.8.1-0lambda0.22.04.1                all          Library of cooperative threadblock primitives for CUDA kernal programming
ii  libcudart12:amd64                                12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Runtime Library
ii  libcupti-dev:amd64                               12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Profiler Tools Interface development files
ii  libcupti12:amd64                                 12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Profiler Tools Interface runtime library
ii  libnvrtc-builtins12.8:amd64                      12.8.93~12.8.1-0lambda0.22.04.1              amd64        CUDA Runtime Compilation (NVIDIA NVRTC Builtins Library)
ii  libnvrtc12:amd64                                 12.8.93~12.8.1-0lambda0.22.04.1              amd64        CUDA Runtime Compilation (NVIDIA NVRTC Library)
ii  nvidia-cuda-dev:amd64                            12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA development files
ii  nvidia-cuda-toolkit                              12.8.93~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA development toolkit
ii  nvidia-cuda-toolkit-gcc                          12.8.1-0lambda0.22.04.1                      amd64        NVIDIA CUDA development toolkit (GCC compatibility)
ii  nvidia-profiler                                  12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA Profiler for CUDA and OpenCL
ii  python3-jax-cuda                                 0.6.0-0lambda0.22.04.1                       all          Metapackage for JAX with CUDA support (Python 3)
ii  python3-jax-cuda12-pjrt                          0.6.0-0lambda0.22.04.1                       amd64        JAX CUDA PJRT runtime plugin (Python 3, CUDA 12)
ii  python3-jax-cuda12-plugin                        0.6.0-0lambda0.22.04.1                       amd64        JAX CUDA kernels plugin (Python 3, CUDA 12)
ii  python3-tensorflow-cuda                          2.19.0-0lambda0.22.04.1                      amd64        Open-source software library for Machine Intelligence (Python 3, CUDA)
ii  python3-torch-cuda                               2.7.0+ds-0lambda0.22.04.1                    amd64        Tensors and Dynamic neural networks GPU accelerated (Python 3)
ii  python3-torchvision-cuda                         0.22.0+ds-0lambda0.22.04.1                   amd64        Image and video datasets and models for PyTorch (Python 3, CUDA)
ii  python3-triton-cuda                              3.3.0+llvm20.1.2-0lambda0.22.04.1            amd64        Language and compiler for parallel programming (Python3, CUDA)

--- NVIDIA Container Toolkit ---

[CMD] nvidia-container-cli: nvidia-container-cli info
NVRM version:   570.148.08
CUDA version:   12.8

Device Index:   0
Device Minor:   0
Model:          NVIDIA B200
Brand:          Nvidia
GPU UUID:       GPU-8741c41e-32f5-8eb1-fa98-6ab21942fa2f
Bus Location:   00000000:06:00.0
Architecture:   10.0

[FILE] /etc/nvidia-container-runtime/config.toml:
#accept-nvidia-visible-devices-as-volume-mounts = false
#accept-nvidia-visible-devices-envvar-when-unprivileged = true
disable-require = false
supported-driver-capabilities = "compat32,compute,display,graphics,ngx,utility,video"
#swarm-resource = "DOCKER_RESOURCE_GPU"

[nvidia-container-cli]
#debug = "/var/log/nvidia-container-toolkit.log"
environment = []
#ldcache = "/etc/ld.so.cache"
ldconfig = "@/sbin/ldconfig.real"
load-kmods = true
#no-cgroups = false
#path = "/usr/bin/nvidia-container-cli"
#root = "/run/nvidia/driver"
#user = "root:video"

[nvidia-container-runtime]
#debug = "/var/log/nvidia-container-runtime.log"
log-level = "info"
mode = "auto"
runtimes = ["docker-runc", "runc", "crun"]

[nvidia-container-runtime.modes]

[nvidia-container-runtime.modes.cdi]
annotation-prefixes = ["cdi.k8s.io/"]
default-kind = "nvidia.com/gpu"
spec-dirs = ["/etc/cdi", "/var/run/cdi"]

[nvidia-container-runtime.modes.csv]
mount-spec-path = "/etc/nvidia-container-runtime/host-files-for-container.d"

[nvidia-container-runtime.modes.legacy]
cuda-compat-mode = "ldconfig"

[nvidia-container-runtime-hook]
path = "nvidia-container-runtime-hook"
skip-mode-detection = false

[nvidia-ctk]
path = "nvidia-ctk"


--- Environment Variables ---

DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin
[CMD] CUDA env vars: env

===============================================================================
  PHASE 7: ML FRAMEWORKS - BLACKWELL COMPATIBILITY
  2025-12-18 20:59:49
===============================================================================


--- Python Environments ---

[CMD] Python version: python3 --version
Python 3.10.12

[CMD] Python path: which python3
/usr/bin/python3

[CMD] Pip version: pip3 --version
pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10)

[CMD] Conda info: conda info
[SKIP] Command not found: conda

[CMD] Conda envs: conda env list
[SKIP] Command not found: conda


--- PyTorch Blackwell/SM_100 Compatibility Check ---

[CRITICAL] B200 requires PyTorch with SM_100 (compute capability 10.0) support
[CRITICAL] Standard PyTorch releases do NOT support B200 - need nightly or NGC builds

PyTorch version: 2.7.0
CUDA available: True
CUDA version (torch): 12.8
cuDNN version: 90800
GPU count: 1

CUDA arch list: ['sm_52', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_89', 'sm_90', 'sm_100', 'sm_120', 'compute_120']
[OK] SM_100 (Blackwell) support detected in PyTorch

GPU 0: NVIDIA B200
  Compute capability: 10.0
  [OK] Blackwell architecture confirmed (SM 10.0)
  Total memory: 178.4 GB
  Multi-processor count: 148

--- TensorFlow ---

TensorFlow version: 2.19.0
GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]
Built with CUDA: True

--- JAX ---

JAX version: 0.6.0
Devices: [CudaDevice(id=0)]
Default backend: gpu

--- vLLM Blackwell Support ---

[INFO] vLLM on B200 requires NGC PyTorch or nightly builds
[INFO] Recent vLLM includes CUTLASS attention (3.6x faster on B200)

[SKIP] vLLM not available

--- TensorRT / TensorRT-LLM ---

TensorRT import failed: No module named 'tensorrt'
TensorRT-LLM not found

--- FlashAttention / FlashInfer (Blackwell Optimized) ---

flash-attn not installed
flashinfer not installed (provides Blackwell-optimized attention)

--- Other ML Libraries ---

Installed ML libraries:
  triton: 3.3.0
  numpy: 1.21.5
  scipy: 1.8.0
  pandas: 1.3.5
  tensorboard: 2.19.0

--- Full Pip Package List ---

[CMD] Pip list: pip3 list
Package                  Version
------------------------ ------------------------------------
absl-py                  2.1.0
appdirs                  1.4.4
argcomplete              1.8.1
astunparse               1.6.3
attrs                    21.2.0
Automat                  20.2.0
Babel                    2.8.0
backcall                 0.2.0
bcrypt                   3.2.0
beautifulsoup4           4.10.0
beniget                  0.4.1
bleach                   4.1.0
blinker                  1.4
bottle                   0.12.19
Brotli                   1.0.9
certifi                  2020.6.20
cffi                     1.15.0
chardet                  4.0.0
click                    8.0.3
cloud-init               25.1.2
colorama                 0.4.4
command-not-found        0.3
commonmark               0.9.1
configobj                5.0.6
constantly               15.1.0
cryptography             3.4.8
ctop                     1.0.0
cycler                   0.11.0
dbus-python              1.2.18
decorator                4.4.2
defusedxml               0.7.1
distlib                  0.3.4
distro                   1.7.0
distro-info              1.1+ubuntu0.2
docker                   5.0.3
entrypoints              0.4
filelock                 3.6.0
flake8                   4.0.1
flatbuffers              1.12.1-git20200711.33e2d80-dfsg1-0.6
fonttools                4.29.1
fs                       2.4.12
fsspec                   2024.3.1
future                   0.18.2
gast                     0.5.2
Glances                  3.2.4.2
google-pasta             0.2.0
grpcio                   1.30.2
h5py                     3.6.0
h5py.-debian-h5py-serial 3.6.0
html5lib                 1.1
httplib2                 0.20.2
hyperlink                21.0.0
icdiff                   2.0.4
idna                     3.3
importlib-metadata       4.6.4
incremental              21.3.0
influxdb                 5.3.1
iotop                    0.6
ipykernel                6.7.0
ipython                  7.31.1
ipython_genutils         0.2.0
jax                      0.6.0
jax-cuda12-pjrt          0.6.0
jax-cuda12-plugin        0.6.0
jaxlib                   0.6.0
jedi                     0.18.0
jeepney                  0.7.1
Jinja2                   3.0.3
joblib                   0.17.0
jsonpatch                1.32
jsonpointer              2.0
jsonschema               3.2.0
jupyter-client           7.1.2
jupyter-core             4.9.1
kaptan                   0.5.12
keras                    3.10.0
keyring                  23.5.0
kiwisolver               1.3.2
launchpadlib             1.10.16
lazr.restfulclient       0.14.4
lazr.uri                 1.0.6
libtmux                  0.10.1
livereload               2.6.3
lxml                     4.8.0
lz4                      3.1.3+dfsg
Markdown                 3.3.6
MarkupSafe               2.0.1
matplotlib               3.5.1
matplotlib-inline        0.1.3
mccabe                   0.6.1
mkdocs                   1.1.2
ml-dtypes                0.5.1
more-itertools           8.10.0
mpmath                   0.0.0
msgpack                  1.0.3
namex                    0.0.8
nest-asyncio             1.5.4
jax-cuda12-pjrt          0.6.0
jax-cuda12-plugin        0.6.0
nvidia-ml-py             12.555.43
tensorboard              2.19.0
tensorflow               2.19.0
torch                    2.7.0
torchvision              0.22.0
triton                   3.3.0

===============================================================================
  PHASE 8: CONTAINER AND ORCHESTRATION
  2025-12-18 21:00:05
===============================================================================


--- Docker ---

[CMD] Docker version: docker version
Client: Docker Engine - Community
 Version:           28.3.1
 API version:       1.51
 Go version:        go1.24.4
 Git commit:        38b7060
 Built:             Wed Jul  2 20:56:22 2025
 OS/Arch:           linux/amd64
 Context:           default
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.51/version": dial unix /var/run/docker.sock: connect: permission denied
[WARN] Command returned non-zero exit code

[CMD] Docker info: docker info
Client: Docker Engine - Community
 Version:    28.3.1
 Context:    default
 Debug Mode: false
 Plugins:
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.25.0
    Path:     /usr/libexec/docker/cli-plugins/docker-buildx
  compose: Docker Compose (Docker Inc.)
    Version:  v2.38.1
    Path:     /usr/libexec/docker/cli-plugins/docker-compose

Server:
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.51/info": dial unix /var/run/docker.sock: connect: permission denied
[WARN] Command returned non-zero exit code

[SKIP] File/dir not found: /etc/docker/daemon.json

[CHECK] Docker default runtime:
[CMD] Docker images: docker images
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Head "http://%2Fvar%2Frun%2Fdocker.sock/_ping": dial unix /var/run/docker.sock: connect: permission denied
[WARN] Command returned non-zero exit code

[CMD] Running containers: docker ps
permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get "http://%2Fvar%2Frun%2Fdocker.sock/v1.51/containers/json": dial unix /var/run/docker.sock: connect: permission denied
[WARN] Command returned non-zero exit code


--- Podman (Alternative Container Runtime) ---

[CMD] Podman version: podman version
Version:      3.4.4
API Version:  3.4.4
Go Version:   go1.18.1
Built:        Thu Jan  1 00:00:00 1970
OS/Arch:      linux/amd64

[CMD] Podman info: podman info
host:
  arch: amd64
  buildahVersion: 1.23.1
  cgroupControllers:
  - memory
  - pids
  cgroupManager: systemd
  cgroupVersion: v2
  conmon:
    package: 'conmon: /usr/bin/conmon'
    path: /usr/bin/conmon
    version: 'conmon version 2.0.25, commit: unknown'
  cpus: 26
  distribution:
    codename: jammy
    distribution: ubuntu
    version: "22.04"
  eventLogger: journald
  hostname: 192-9-179-49
  idMappings:
    gidmap:
    - container_id: 0
      host_id: 1000
      size: 1
    - container_id: 1
      host_id: 100000
      size: 65536
    uidmap:
    - container_id: 0

--- Kubernetes ---

[CMD] kubectl version: kubectl version --client
[SKIP] Command not found: kubectl


--- Slurm ---

[CMD] Slurm version: sinfo --version
[SKIP] Command not found: sinfo

[CMD] Slurm nodes: sinfo
[SKIP] Command not found: sinfo


===============================================================================
  PHASE 9: PERFORMANCE BASELINE TESTS
  2025-12-18 21:00:06
===============================================================================


--- Disk I/O Baseline (Model Loading Speed) ---

[INFO] Testing disk write/read speed (affects model loading time)



--- GPU Utilization Baseline ---

[CMD] GPU stats 5 samples: nvidia-smi dmon -c 5 -s pucvmet
# gpu    pwr  gtemp  mtemp     sm    mem    enc    dec    jpg    ofa   mclk   pclk  pviol  tviol     fb   bar1   ccpm  sbecc  dbecc    pci  rxpci  txpci 
# Idx      W      C      C      %      %      %      %      %      %    MHz    MHz      %   bool     MB     MB     MB   errs   errs   errs   MB/s   MB/s 
    0    141     33     33      0      0      0      0      0      0   3996    195      0      0      0      1      0      0      0      0      0      0 
    0    141     33     33      0      0      0      0      0      0   3996    120    100      0      0      1      0      0      0      0      0      0 
    0    141     33     33      0      0      0      0      0      0   3996    120    100      0      0      1      0      0      0      0      0      0 
    0    141     33     33      0      0      0      0      0      0   3996    120    100      0      0      1      0      0      0      0      0      1 
    0    141     33     33      0      0      0      0      0      0   3996    120    100      0      0      1      0      0      0      0      0      0 


--- GPU Memory Bandwidth Test ---

Testing on: NVIDIA B200
Memory bandwidth (1GB clone): ~2562.3 GB/s
[INFO] B200 HBM3e theoretical: ~8000 GB/s

--- FP16 Matrix Multiplication (Transformer Workload) ---

FP16 MatMul (8192x8192): 1566.9 TFLOPS
[INFO] B200 theoretical FP16: ~2250 TFLOPS (with sparsity)

--- NVLink Bandwidth Test (Multi-GPU) ---

Only 1 GPU(s), need 2+ for NVLink test

===============================================================================
  PHASE 10: ADDITIONAL DISCOVERY
  2025-12-18 21:00:17
===============================================================================


--- Pre-loaded Models ---

[CMD] Hugging Face cache: ls -la /home/ubuntu/.cache/huggingface/
ls: cannot access '/home/ubuntu/.cache/huggingface/': No such file or directory
[WARN] Command returned non-zero exit code

[CMD] HF hub models: ls /home/ubuntu/.cache/huggingface/hub/
ls: cannot access '/home/ubuntu/.cache/huggingface/hub/': No such file or directory
[WARN] Command returned non-zero exit code

[CMD] Torch hub cache: ls -la /home/ubuntu/.cache/torch/
ls: cannot access '/home/ubuntu/.cache/torch/': No such file or directory
[WARN] Command returned non-zero exit code


--- Lambda Stack Specific ---

[CMD] Lambda Stack packages: dpkg -l
ii  cudnn-license                                    9.8.0.87-0lambda1                            all          NVIDIA CUDA deep neural network, run-time libraries (license)
ii  lambda-repository                                0.1                                          all          Repository for Lambda Labs software
ii  lambda-stack-cuda                                0.1.17~22.04.1                               all          Deep learning software stack from Lambda Labs (CUDA)
ii  libaccinj64-12.8:amd64                           12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA ACCINJ Library (64-bit)
ii  libcu++-dev                                      2.7.0~12.8.1-0lambda0.22.04.1                all          NVIDIA C++ Standard Library
ii  libcub-dev                                       2.7.0~12.8.1-0lambda0.22.04.1                all          Library of cooperative threadblock primitives for CUDA kernal programming
ii  libcublas12:amd64                                12.8.4.1~12.8.1-0lambda0.22.04.1             amd64        NVIDIA cuBLAS Library
ii  libcublaslt12:amd64                              12.8.4.1~12.8.1-0lambda0.22.04.1             amd64        NVIDIA cuBLASLt Library
ii  libcudart12:amd64                                12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Runtime Library
ii  libcufft11:amd64                                 11.3.3.83~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuFFT Library
ii  libcufftw11:amd64                                11.3.3.83~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuFFTW Library
ii  libcufile0:amd64                                 1.13.1.3~12.8.1-0lambda0.22.04.1             amd64        GPUDirect Storage cuFile runtime library
ii  libcuinj64-12.8:amd64                            12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUINJ Library (64-bit)
ii  libcupti-dev:amd64                               12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Profiler Tools Interface development files
ii  libcupti12:amd64                                 12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA Profiler Tools Interface runtime library
ii  libcurand10:amd64                                11.1.1+~10.3.9.90~12.8.1-0lambda0.22.04.1    amd64        NVIDIA cuRAND Library
ii  libcusolver11:amd64                              11.7.3.90~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuSOLVER Library
ii  libcusolvermg11:amd64                            11.7.3.90~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuSOLVERmg Library
ii  libcusparse12:amd64                              12.5.8.93~12.8.1-0lambda0.22.04.1            amd64        NVIDIA cuSPARSE Library
ii  libmagma2:amd64                                  2.9.0+ds-0lambda1                            amd64        MAGMA linear algebra library (runtime)
ii  libnccl-dev:amd64                                2.26.2-0lambda1                              amd64        NVIDIA Collectives Communication Library (NCCL) development files
ii  libnccl2:amd64                                   2.26.2-0lambda1                              amd64        NVIDIA Collectives Communication Library (NCCL) runtime
ii  libnppc12:amd64                                  12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives core runtime library
ii  libnppial12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Arithmetic and Logic
ii  libnppicc12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Color Conversion
ii  libnppidei12:amd64                               12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Data Exchange and Initialization
ii  libnppif12:amd64                                 12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Filters
ii  libnppig12:amd64                                 12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Geometry transforms
ii  libnppim12:amd64                                 12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Morphological operations
ii  libnppist12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Statistics
ii  libnppisu12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Support
ii  libnppitc12:amd64                                12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives lib for Image Threshold and Compare
ii  libnpps12:amd64                                  12.3.3.100~12.8.1-0lambda0.22.04.1           amd64        NVIDIA Performance Primitives for signal processing runtime library
ii  libnvblas12:amd64                                12.8.4.1~12.8.1-0lambda0.22.04.1             amd64        NVBLAS runtime library
ii  libnvidia-cfg1-570-server:amd64                  570.148.08-0lambda0.22.04.1                  amd64        NVIDIA binary OpenGL/GLX configuration library
ii  libnvidia-compute-570-server:amd64               570.148.08-0lambda0.22.04.1                  amd64        NVIDIA libcompute package
ii  libnvidia-container-tools                        1.17.8+dfsg-0lambda0.22.04.1                 amd64        Package for configuring containers with NVIDIA hardware (CLI tool)
ii  libnvidia-container1:amd64                       1.17.8+dfsg-0lambda0.22.04.1                 amd64        Package for configuring containers with NVIDIA hardware (shared library)
ii  libnvidia-extra-570-server:amd64                 570.148.08-0lambda0.22.04.1                  amd64        Extra libraries for the NVIDIA driver
ii  libnvidia-ml-dev:amd64                           12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA Management Library (NVML) development files
ii  libnvjitlink12:amd64                             12.8.93~12.8.1-0lambda0.22.04.1              amd64        NVIDIA Compiler JIT LTO Library
ii  libnvjpeg12:amd64                                12.3.5.92~12.8.1-0lambda0.22.04.1            amd64        NVIDIA JPEG library (nvJPEG)
ii  libnvrtc-builtins12.8:amd64                      12.8.93~12.8.1-0lambda0.22.04.1              amd64        CUDA Runtime Compilation (NVIDIA NVRTC Builtins Library)
ii  libnvrtc12:amd64                                 12.8.93~12.8.1-0lambda0.22.04.1              amd64        CUDA Runtime Compilation (NVIDIA NVRTC Library)
ii  libnvsdm-570                                     570.148.08-0lambda0.22.04.1                  amd64        NVIDIA Switch Device Monitoring (570 series)
ii  libnvtoolsext1:amd64                             12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA Tools Extension Library
ii  libnvvm4:amd64                                   12.8.93~12.8.1-0lambda0.22.04.1              amd64        NVIDIA NVVM Library
ii  libprotobuf-dev:amd64                            3.21.12-6ubuntu1~lambda22.04.2               amd64        protocol buffers C++ library (development files) and proto files
ii  libprotobuf-lite32:amd64                         3.21.12-6ubuntu1~lambda22.04.2               amd64        protocol buffers C++ library (lite version)
ii  libprotobuf32:amd64                              3.21.12-6ubuntu1~lambda22.04.2               amd64        protocol buffers C++ library
ii  libthrust-dev                                    2.7.0~12.8.1-0lambda0.22.04.1                all          Thrust - Parallel Algorithms Library
ii  nccl-tests                                       2.14.1-0lambda2                              amd64        NVIDIA Collectives Communication Library (NCCL) performance tests
ii  nccl-tools                                       2.26.2-0lambda1                              amd64        NVIDIA Collectives Communication Library (NCCL) tools
ii  nvidia-compute-utils-570-server                  570.148.08-0lambda0.22.04.1                  amd64        NVIDIA compute utilities
ii  nvidia-container-toolkit                         1.17.8-0lambda0.22.04.1                      amd64        OCI hook for configuring containers for NVIDIA hardware
ii  nvidia-container-toolkit-base                    1.17.8-0lambda0.22.04.1                      amd64        Container runtime for NVIDIA hardware
ii  nvidia-cuda-dev:amd64                            12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA development files
ii  nvidia-cuda-toolkit                              12.8.93~12.8.1-0lambda0.22.04.1              amd64        NVIDIA CUDA development toolkit
ii  nvidia-cuda-toolkit-gcc                          12.8.1-0lambda0.22.04.1                      amd64        NVIDIA CUDA development toolkit (GCC compatibility)
ii  nvidia-dkms-570-server-open                      570.148.08-0lambda0.22.04.1                  amd64        NVIDIA DKMS package (open module)
ii  nvidia-fabricmanager-570                         570.148.08-0lambda0.22.04.1                  amd64        NVIDIA Fabric Manager for NVSwitch systems
ii  nvidia-firmware-570-server-570.148.08            570.148.08-0lambda0.22.04.1                  amd64        Firmware files used by the kernel module
ii  nvidia-headless-570-server-open                  570.148.08-0lambda0.22.04.1                  amd64        NVIDIA Server headless metapackage
ii  nvidia-headless-no-dkms-570-server-open          570.148.08-0lambda0.22.04.1                  amd64        NVIDIA Server headless metapackage - no DKMS (open kernel module)
ii  nvidia-kernel-common-570-server                  570.148.08-0lambda0.22.04.1                  amd64        Shared files used with the kernel module
ii  nvidia-kernel-source-570-server-open             570.148.08-0lambda0.22.04.1                  amd64        NVIDIA kernel source package (open)
ii  nvidia-opencl-dev:amd64                          12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA OpenCL development files
ii  nvidia-profiler                                  12.8.90~12.8.1-0lambda0.22.04.1              amd64        NVIDIA Profiler for CUDA and OpenCL
ii  nvidia-utils-570-server                          570.148.08-0lambda0.22.04.1                  amd64        NVIDIA driver support binaries
ii  nvlsm                                            2025.03.1-0lambda0.22.04.2                   amd64        NVIDIA NVLink Subnet Manager
ii  python3-absl                                     2.1.0-0lambda1                               all          Abseil Python common libraries (Python 3)
ii  python3-fsspec                                   2024.3.1-0lambda1                            all          specification that Python filesystems should adhere to (Python 3)
ii  python3-grpcio                                   1.30.2-3lambda22.04.1                        amd64        GRPC system (Python 3)
ii  python3-jax                                      0.6.0-0lambda0.22.04.1                       all          High-Performance Array Computing (Python 3)
ii  python3-jax-cuda                                 0.6.0-0lambda0.22.04.1                       all          Metapackage for JAX with CUDA support (Python 3)
ii  python3-jax-cuda12-pjrt                          0.6.0-0lambda0.22.04.1                       amd64        JAX CUDA PJRT runtime plugin (Python 3, CUDA 12)
ii  python3-jax-cuda12-plugin                        0.6.0-0lambda0.22.04.1                       amd64        JAX CUDA kernels plugin (Python 3, CUDA 12)
ii  python3-jaxlib                                   0.6.0-0lambda0.22.04.1                       amd64        Support library for JAX (Python 3)
ii  python3-keras                                    3.10.0-0lambda1                              all          High-level neural networks API (Python 3)
ii  python3-ml-dtypes                                0.5.1+eigen20230307.7bf2968-0lambda0.22.04.1 amd64        NumPy dtype extensions for machine learning (Python 3)
ii  python3-namex                                    0.0.8-0lambda1                               all          Namex: Clean up the public namespace of your package (Python 3)
ii  python3-opt-einsum                               3.3.0-0lambda1                               all          Tensor contraction order optimizer (Python 3)
ii  python3-optree                                   0.15.0-0lambda0.22.04.1                      amd64        Optimized PyTree utilities (Python 3)
ii  python3-pasta                                    0.2.0-0lambda1                               all          Python AST augmentation (Python 3 module)
ii  python3-protobuf                                 3.21.12-6ubuntu1~lambda22.04.2               amd64        Python 3 bindings for protocol buffers
ii  python3-pynvml                                   12.555.43-0lambda1                           all          Python bindings to the NVIDIA Management Library (Python 3)
ii  python3-sympy                                    1.12-7~lambda1                               all          Computer Algebra System (CAS) in Python (Python 3)
ii  python3-tensorboard                              2.19.0-0lambda1                              all          Visualization toolkit for TensorFlow (Python 3)
ii  python3-tensorflow-cuda                          2.19.0-0lambda0.22.04.1                      amd64        Open-source software library for Machine Intelligence (Python 3, CUDA)
ii  python3-torch-cuda                               2.7.0+ds-0lambda0.22.04.1                    amd64        Tensors and Dynamic neural networks GPU accelerated (Python 3)
ii  python3-torchvision-cuda                         0.22.0+ds-0lambda0.22.04.1                   amd64        Image and video datasets and models for PyTorch (Python 3, CUDA)
ii  python3-triton                                   3.3.0+llvm20.1.2-0lambda0.22.04.1            amd64        Language and compiler for parallel programming (Python3)
ii  python3-triton-cuda                              3.3.0+llvm20.1.2-0lambda0.22.04.1            amd64        Language and compiler for parallel programming (Python3, CUDA)
ii  tensorboard                                      2.19.0-0lambda1                              all          Visualization toolkit for TensorFlow (command line)
[SKIP] File/dir not found: /etc/lambda-stack-version


--- Brev Specific ---

[INFO] Brev (acquired by NVIDIA mid-2024) acts as multi-cloud GPU aggregator
[CMD] Brev CLI: brev version
[SKIP] Command not found: brev

[CMD] Brev workspace info: brev ls
[SKIP] Command not found: brev

[SKIP] File/dir not found: /home/ubuntu/.brev/config.yaml

[SKIP] File/dir not found: /home/ubuntu/.brev

[CMD] Brev agent processes: pgrep -a brev
[WARN] Command returned non-zero exit code

[CMD] Brev services: systemctl list-units
[CMD] Brev directories: ls -la /opt/brev* /etc/brev* /home/ubuntu/.brev*
ls: cannot access '/opt/brev*': No such file or directory
ls: cannot access '/etc/brev*': No such file or directory
ls: cannot access '/home/ubuntu/.brev*': No such file or directory
[WARN] Command returned non-zero exit code

[CMD] SSH authorized keys (Brev injected): cat /home/ubuntu/.ssh/authorized_keys
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDUao6kcz0zmJ4Rtn0vbUc0VQS8a7Hbdpwg0ORuuc6p6MSb2aEYSXp8pi900f6O


--- Cloud Provider Backend Detection ---

[INFO] Detecting underlying cloud provider (Lambda, CoreWeave, etc.)
[CMD] Cloud metadata check: curl -s --connect-timeout 2 http://169.254.169.254/latest/meta-data/instance-type
<html>
<head><title>404 Not Found</title></head>
<body bgcolor="white">
<center><h1>404 Not Found</h1></center>
</body>
</html>

[INFO] Hostname pattern suggests Lambda Labs IP-based naming

--- Jupyter Environment ---

[CMD] JupyterLab: jupyter lab --version
[SKIP] Command not found: jupyter

[CMD] Jupyter kernels: jupyter kernelspec list
[SKIP] Command not found: jupyter


--- System Services (GPU-related) ---

[CMD] GPU services: systemctl list-units --type=service
  docker.service                                 loaded active running Docker Application Container Engine
  nvidia-persistenced.service                    loaded active running NVIDIA Persistence Daemon

--- Kernel Modules ---

[CMD] Loaded nvidia modules: lsmod
nvidia_uvm           2121728  0
nvidia_drm            131072  0
nvidia_modeset       1724416  1 nvidia_drm
video                  77824  1 nvidia_modeset
nvidia_peermem         16384  0
ib_uverbs             200704  3 nvidia_peermem,rdma_ucm,mlx5_ib
nvidia              11644928  8 nvidia_uvm,nvidia_peermem,nvidia_modeset
ecc                    45056  1 nvidia
112

--- System Limits ---

[CMD] ulimit -a: ulimit -a
real-time non-blocking time  (microseconds, -R) unlimited
core file size              (blocks, -c) 0
data seg size               (kbytes, -d) unlimited
scheduling priority                 (-e) 0
file size                   (blocks, -f) unlimited
pending signals                     (-i) 1449442
max locked memory           (kbytes, -l) unlimited
max memory size             (kbytes, -m) unlimited
open files                          (-n) 1024
pipe size                (512 bytes, -p) 8
POSIX message queues         (bytes, -q) 819200
real-time priority                  (-r) 0
stack size                  (kbytes, -s) 8192
cpu time                   (seconds, -t) unlimited
max user processes                  (-u) 1449442
virtual memory              (kbytes, -v) unlimited
file locks                          (-x) unlimited

[CMD] Max open files: cat /proc/sys/fs/file-max
9223372036854775807


--- Instance Metadata (Cloud) ---

[CMD] Cloud provider metadata: curl -s --connect-timeout 2 http://169.254.169.254/latest/meta-data/
<html>
<head><title>404 Not Found</title></head>
<body bgcolor="white">
<center><h1>404 Not Found</h1></center>
</body>
</html>


===============================================================================
  DISCOVERY COMPLETE
  2025-12-18 21:00:17
===============================================================================


===============================================================================
  DISCOVERY COMPLETE
  Finished at: Thu Dec 18 21:00:17 UTC 2025
  Log file: /home/ubuntu/b200_discovery_192-9-179-49_20251218_205947.log
===============================================================================

=== QUICK SUMMARY ===

System: 192-9-179-49 - 6.8.0-60-generic
OS: Ubuntu 22.04.5 LTS

name, memory.total [MiB], driver_version
NVIDIA B200, 183359 MiB, 570.148.08

CUDA Toolkit: 12.8
PyTorch: 2.7.0
Python: 3.10.12

=== B200 BLACKWELL VALIDATION ===
[CHECK] Verify nvidia-open driver
[CHECK] Fabric Manager status
[OK] PyTorch SM_100 support

Log saved to: /home/ubuntu/b200_discovery_192-9-179-49_20251218_205947.log
To compare instances: diff <log1> <log2>
